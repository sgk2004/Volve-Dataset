{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>WELL_BORE_CODE</th>\n",
       "      <th>AVG_DOWNHOLE_PRESSURE</th>\n",
       "      <th>AVG_DOWNHOLE_TEMPERATURE</th>\n",
       "      <th>AVG_CHOKE_SIZE_P</th>\n",
       "      <th>AVG_WHP_P</th>\n",
       "      <th>AVG_WHT_P</th>\n",
       "      <th>DP_CHOKE_SIZE</th>\n",
       "      <th>BORE_OIL_VOL</th>\n",
       "      <th>BORE_GAS_VOL</th>\n",
       "      <th>BORE_WAT_VOL</th>\n",
       "      <th>FLOW_KIND</th>\n",
       "      <th>WELL_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>NO 15/9-F-1 C</td>\n",
       "      <td>289.42</td>\n",
       "      <td>106.35</td>\n",
       "      <td>43.34</td>\n",
       "      <td>107.36</td>\n",
       "      <td>37.94</td>\n",
       "      <td>78.94</td>\n",
       "      <td>631.47</td>\n",
       "      <td>90439.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>production</td>\n",
       "      <td>OP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>NO 15/9-F-1 C</td>\n",
       "      <td>270.24</td>\n",
       "      <td>107.64</td>\n",
       "      <td>47.17</td>\n",
       "      <td>99.19</td>\n",
       "      <td>60.76</td>\n",
       "      <td>70.63</td>\n",
       "      <td>1166.46</td>\n",
       "      <td>165720.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>production</td>\n",
       "      <td>OP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>NO 15/9-F-1 C</td>\n",
       "      <td>262.84</td>\n",
       "      <td>107.87</td>\n",
       "      <td>47.73</td>\n",
       "      <td>94.60</td>\n",
       "      <td>63.05</td>\n",
       "      <td>66.05</td>\n",
       "      <td>1549.81</td>\n",
       "      <td>221707.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>production</td>\n",
       "      <td>OP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>NO 15/9-F-1 C</td>\n",
       "      <td>255.53</td>\n",
       "      <td>107.97</td>\n",
       "      <td>48.53</td>\n",
       "      <td>89.99</td>\n",
       "      <td>64.55</td>\n",
       "      <td>61.41</td>\n",
       "      <td>1248.70</td>\n",
       "      <td>178063.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>production</td>\n",
       "      <td>OP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>NO 15/9-F-1 C</td>\n",
       "      <td>247.20</td>\n",
       "      <td>108.05</td>\n",
       "      <td>49.84</td>\n",
       "      <td>84.78</td>\n",
       "      <td>65.72</td>\n",
       "      <td>56.15</td>\n",
       "      <td>1345.78</td>\n",
       "      <td>192602.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>production</td>\n",
       "      <td>OP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5938</th>\n",
       "      <td>8923</td>\n",
       "      <td>NO 15/9-F-15 D</td>\n",
       "      <td>194.98</td>\n",
       "      <td>106.52</td>\n",
       "      <td>31.58</td>\n",
       "      <td>15.81</td>\n",
       "      <td>49.02</td>\n",
       "      <td>1.26</td>\n",
       "      <td>144.01</td>\n",
       "      <td>23201.35</td>\n",
       "      <td>203.93</td>\n",
       "      <td>production</td>\n",
       "      <td>OP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5939</th>\n",
       "      <td>8924</td>\n",
       "      <td>NO 15/9-F-15 D</td>\n",
       "      <td>194.98</td>\n",
       "      <td>106.52</td>\n",
       "      <td>31.54</td>\n",
       "      <td>15.77</td>\n",
       "      <td>48.99</td>\n",
       "      <td>1.20</td>\n",
       "      <td>145.22</td>\n",
       "      <td>23068.07</td>\n",
       "      <td>202.93</td>\n",
       "      <td>production</td>\n",
       "      <td>OP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5940</th>\n",
       "      <td>8925</td>\n",
       "      <td>NO 15/9-F-15 D</td>\n",
       "      <td>194.32</td>\n",
       "      <td>106.52</td>\n",
       "      <td>31.52</td>\n",
       "      <td>15.70</td>\n",
       "      <td>50.10</td>\n",
       "      <td>1.28</td>\n",
       "      <td>142.74</td>\n",
       "      <td>23059.68</td>\n",
       "      <td>203.84</td>\n",
       "      <td>production</td>\n",
       "      <td>OP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5941</th>\n",
       "      <td>8926</td>\n",
       "      <td>NO 15/9-F-15 D</td>\n",
       "      <td>195.21</td>\n",
       "      <td>106.51</td>\n",
       "      <td>31.52</td>\n",
       "      <td>15.61</td>\n",
       "      <td>49.84</td>\n",
       "      <td>1.20</td>\n",
       "      <td>144.46</td>\n",
       "      <td>23090.47</td>\n",
       "      <td>202.76</td>\n",
       "      <td>production</td>\n",
       "      <td>OP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5942</th>\n",
       "      <td>8927</td>\n",
       "      <td>NO 15/9-F-15 D</td>\n",
       "      <td>195.31</td>\n",
       "      <td>106.51</td>\n",
       "      <td>24.92</td>\n",
       "      <td>15.76</td>\n",
       "      <td>48.73</td>\n",
       "      <td>1.30</td>\n",
       "      <td>106.30</td>\n",
       "      <td>17537.08</td>\n",
       "      <td>155.70</td>\n",
       "      <td>production</td>\n",
       "      <td>OP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5943 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  WELL_BORE_CODE  AVG_DOWNHOLE_PRESSURE  \\\n",
       "0             15   NO 15/9-F-1 C                 289.42   \n",
       "1             16   NO 15/9-F-1 C                 270.24   \n",
       "2             17   NO 15/9-F-1 C                 262.84   \n",
       "3             18   NO 15/9-F-1 C                 255.53   \n",
       "4             19   NO 15/9-F-1 C                 247.20   \n",
       "...          ...             ...                    ...   \n",
       "5938        8923  NO 15/9-F-15 D                 194.98   \n",
       "5939        8924  NO 15/9-F-15 D                 194.98   \n",
       "5940        8925  NO 15/9-F-15 D                 194.32   \n",
       "5941        8926  NO 15/9-F-15 D                 195.21   \n",
       "5942        8927  NO 15/9-F-15 D                 195.31   \n",
       "\n",
       "      AVG_DOWNHOLE_TEMPERATURE  AVG_CHOKE_SIZE_P  AVG_WHP_P  AVG_WHT_P  \\\n",
       "0                       106.35             43.34     107.36      37.94   \n",
       "1                       107.64             47.17      99.19      60.76   \n",
       "2                       107.87             47.73      94.60      63.05   \n",
       "3                       107.97             48.53      89.99      64.55   \n",
       "4                       108.05             49.84      84.78      65.72   \n",
       "...                        ...               ...        ...        ...   \n",
       "5938                    106.52             31.58      15.81      49.02   \n",
       "5939                    106.52             31.54      15.77      48.99   \n",
       "5940                    106.52             31.52      15.70      50.10   \n",
       "5941                    106.51             31.52      15.61      49.84   \n",
       "5942                    106.51             24.92      15.76      48.73   \n",
       "\n",
       "      DP_CHOKE_SIZE  BORE_OIL_VOL  BORE_GAS_VOL  BORE_WAT_VOL   FLOW_KIND  \\\n",
       "0             78.94        631.47      90439.09          0.00  production   \n",
       "1             70.63       1166.46     165720.39          0.00  production   \n",
       "2             66.05       1549.81     221707.31          0.00  production   \n",
       "3             61.41       1248.70     178063.52          0.00  production   \n",
       "4             56.15       1345.78     192602.19          0.00  production   \n",
       "...             ...           ...           ...           ...         ...   \n",
       "5938           1.26        144.01      23201.35        203.93  production   \n",
       "5939           1.20        145.22      23068.07        202.93  production   \n",
       "5940           1.28        142.74      23059.68        203.84  production   \n",
       "5941           1.20        144.46      23090.47        202.76  production   \n",
       "5942           1.30        106.30      17537.08        155.70  production   \n",
       "\n",
       "     WELL_TYPE  \n",
       "0           OP  \n",
       "1           OP  \n",
       "2           OP  \n",
       "3           OP  \n",
       "4           OP  \n",
       "...        ...  \n",
       "5938        OP  \n",
       "5939        OP  \n",
       "5940        OP  \n",
       "5941        OP  \n",
       "5942        OP  \n",
       "\n",
       "[5943 rows x 13 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import cleaned data csv\n",
    "all_wells = pd.read_csv('Data/well_cleaned.csv')\n",
    "all_wells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUTS: AVG_CHOKE_SIZE_P, AVG_WHP_P, AVG_WHT_P, BORE_OIL_VOL, BORE_GAS_VOL, BORE_WAT_VOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5943, 6)\n"
     ]
    }
   ],
   "source": [
    "#read in data for analysis \n",
    "X1= all_wells[[\"AVG_CHOKE_SIZE_P\",\"AVG_WHP_P\",\"AVG_WHT_P\",\"BORE_OIL_VOL\",\"BORE_GAS_VOL\", \"BORE_WAT_VOL\"]]\n",
    "y1= all_wells[\"AVG_DOWNHOLE_PRESSURE\"].values.reshape(-1, 1)\n",
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into test and train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# # Create a StandardScater model and fit it to the training data\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "y_scaler = StandardScaler().fit(y_train)\n",
    "# # Transform the training and testing data using the X_scaler and y_scaler models\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "y_train_scaled = y_scaler.transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the neural network\n",
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the model\n",
    "from tensorflow.keras.layers import Dense\n",
    "number_inputs = X_train.shape[1]\n",
    "number_hidden_nodes = 100\n",
    "\n",
    "model.add(Dense(units=number_hidden_nodes,\n",
    "                activation='relu', input_dim=number_inputs))\n",
    "model.add(Dense(number_hidden_nodes, kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal',activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics\n",
    "# CREDIT: https://github.com/keras-team/keras/issues/7947\n",
    "# root mean squared error (rmse) for regression (only for Keras tensors)\n",
    "def rmse(y_true, y_pred):\n",
    "    from keras import backend\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "# mean squared error (mse) for regression  (only for Keras tensors)\n",
    "def mse(y_true, y_pred):\n",
    "    from keras import backend\n",
    "    return backend.mean(backend.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "# coefficient of determination (R^2) for regression  (only for Keras tensors)\n",
    "def r_square(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "from keras import losses\n",
    "\n",
    "model.compile(loss=\"mean_absolute_error\",\n",
    "              optimizer=\"adam\", metrics=[r_square, rmse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 100)               700       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 10,901\n",
      "Trainable params: 10,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1263 samples, validate on 223 samples\n",
      "Epoch 1/100\n",
      "1263/1263 - 1s - loss: 0.6295 - r_square: 0.2888 - rmse: 0.6295 - val_loss: 0.5027 - val_r_square: 0.4800 - val_rmse: 0.5027\n",
      "Epoch 2/100\n",
      "1263/1263 - 0s - loss: 0.4379 - r_square: 0.5815 - rmse: 0.4379 - val_loss: 0.3834 - val_r_square: 0.6321 - val_rmse: 0.3834\n",
      "Epoch 3/100\n",
      "1263/1263 - 0s - loss: 0.3392 - r_square: 0.7095 - rmse: 0.3392 - val_loss: 0.3203 - val_r_square: 0.6846 - val_rmse: 0.3203\n",
      "Epoch 4/100\n",
      "1263/1263 - 0s - loss: 0.2806 - r_square: 0.7715 - rmse: 0.2806 - val_loss: 0.2980 - val_r_square: 0.6926 - val_rmse: 0.2980\n",
      "Epoch 5/100\n",
      "1263/1263 - 0s - loss: 0.2442 - r_square: 0.8188 - rmse: 0.2442 - val_loss: 0.2839 - val_r_square: 0.6923 - val_rmse: 0.2839\n",
      "Epoch 6/100\n",
      "1263/1263 - 0s - loss: 0.2244 - r_square: 0.8215 - rmse: 0.2244 - val_loss: 0.2767 - val_r_square: 0.6800 - val_rmse: 0.2767\n",
      "Epoch 7/100\n",
      "1263/1263 - 0s - loss: 0.2203 - r_square: 0.8315 - rmse: 0.2203 - val_loss: 0.2647 - val_r_square: 0.6862 - val_rmse: 0.2647\n",
      "Epoch 8/100\n",
      "1263/1263 - 0s - loss: 0.2138 - r_square: 0.8322 - rmse: 0.2138 - val_loss: 0.2555 - val_r_square: 0.6732 - val_rmse: 0.2555\n",
      "Epoch 9/100\n",
      "1263/1263 - 0s - loss: 0.1979 - r_square: 0.8511 - rmse: 0.1979 - val_loss: 0.2399 - val_r_square: 0.6812 - val_rmse: 0.2399\n",
      "Epoch 10/100\n",
      "1263/1263 - 0s - loss: 0.1942 - r_square: 0.8578 - rmse: 0.1942 - val_loss: 0.2391 - val_r_square: 0.6741 - val_rmse: 0.2391\n",
      "Epoch 11/100\n",
      "1263/1263 - 0s - loss: 0.1879 - r_square: 0.8559 - rmse: 0.1879 - val_loss: 0.2352 - val_r_square: 0.6673 - val_rmse: 0.2352\n",
      "Epoch 12/100\n",
      "1263/1263 - 0s - loss: 0.1891 - r_square: 0.8631 - rmse: 0.1891 - val_loss: 0.2317 - val_r_square: 0.6539 - val_rmse: 0.2317\n",
      "Epoch 13/100\n",
      "1263/1263 - 0s - loss: 0.1801 - r_square: 0.8680 - rmse: 0.1801 - val_loss: 0.2258 - val_r_square: 0.6587 - val_rmse: 0.2258\n",
      "Epoch 14/100\n",
      "1263/1263 - 0s - loss: 0.1703 - r_square: 0.8664 - rmse: 0.1703 - val_loss: 0.2160 - val_r_square: 0.6486 - val_rmse: 0.2160\n",
      "Epoch 15/100\n",
      "1263/1263 - 0s - loss: 0.1645 - r_square: 0.8683 - rmse: 0.1645 - val_loss: 0.2178 - val_r_square: 0.6351 - val_rmse: 0.2178\n",
      "Epoch 16/100\n",
      "1263/1263 - 0s - loss: 0.1643 - r_square: 0.8748 - rmse: 0.1643 - val_loss: 0.2248 - val_r_square: 0.6518 - val_rmse: 0.2248\n",
      "Epoch 17/100\n",
      "1263/1263 - 0s - loss: 0.1622 - r_square: 0.8754 - rmse: 0.1622 - val_loss: 0.2124 - val_r_square: 0.6399 - val_rmse: 0.2124\n",
      "Epoch 18/100\n",
      "1263/1263 - 0s - loss: 0.1642 - r_square: 0.8893 - rmse: 0.1642 - val_loss: 0.2053 - val_r_square: 0.6475 - val_rmse: 0.2053\n",
      "Epoch 19/100\n",
      "1263/1263 - 0s - loss: 0.1578 - r_square: 0.8770 - rmse: 0.1578 - val_loss: 0.2192 - val_r_square: 0.6391 - val_rmse: 0.2192\n",
      "Epoch 20/100\n",
      "1263/1263 - 0s - loss: 0.1651 - r_square: 0.8799 - rmse: 0.1651 - val_loss: 0.2041 - val_r_square: 0.6636 - val_rmse: 0.2041\n",
      "Epoch 21/100\n",
      "1263/1263 - 0s - loss: 0.1612 - r_square: 0.8787 - rmse: 0.1612 - val_loss: 0.2150 - val_r_square: 0.6512 - val_rmse: 0.2150\n",
      "Epoch 22/100\n",
      "1263/1263 - 0s - loss: 0.1529 - r_square: 0.8905 - rmse: 0.1529 - val_loss: 0.2109 - val_r_square: 0.6505 - val_rmse: 0.2109\n",
      "Epoch 23/100\n",
      "1263/1263 - 0s - loss: 0.1510 - r_square: 0.8736 - rmse: 0.1510 - val_loss: 0.2009 - val_r_square: 0.6438 - val_rmse: 0.2009\n",
      "Epoch 24/100\n",
      "1263/1263 - 0s - loss: 0.1496 - r_square: 0.8868 - rmse: 0.1496 - val_loss: 0.2005 - val_r_square: 0.6544 - val_rmse: 0.2005\n",
      "Epoch 25/100\n",
      "1263/1263 - 0s - loss: 0.1553 - r_square: 0.8930 - rmse: 0.1553 - val_loss: 0.2049 - val_r_square: 0.6291 - val_rmse: 0.2049\n",
      "Epoch 26/100\n",
      "1263/1263 - 0s - loss: 0.1502 - r_square: 0.8853 - rmse: 0.1502 - val_loss: 0.2009 - val_r_square: 0.6650 - val_rmse: 0.2009\n",
      "Epoch 27/100\n",
      "1263/1263 - 0s - loss: 0.1493 - r_square: 0.8780 - rmse: 0.1493 - val_loss: 0.1989 - val_r_square: 0.6470 - val_rmse: 0.1989\n",
      "Epoch 28/100\n",
      "1263/1263 - 0s - loss: 0.1508 - r_square: 0.8762 - rmse: 0.1508 - val_loss: 0.2059 - val_r_square: 0.6577 - val_rmse: 0.2059\n",
      "Epoch 29/100\n",
      "1263/1263 - 0s - loss: 0.1521 - r_square: 0.8912 - rmse: 0.1521 - val_loss: 0.1976 - val_r_square: 0.6596 - val_rmse: 0.1976\n",
      "Epoch 30/100\n",
      "1263/1263 - 0s - loss: 0.1463 - r_square: 0.8920 - rmse: 0.1463 - val_loss: 0.1974 - val_r_square: 0.6496 - val_rmse: 0.1974\n",
      "Epoch 31/100\n",
      "1263/1263 - 0s - loss: 0.1431 - r_square: 0.8893 - rmse: 0.1431 - val_loss: 0.1959 - val_r_square: 0.6504 - val_rmse: 0.1959\n",
      "Epoch 32/100\n",
      "1263/1263 - 0s - loss: 0.1425 - r_square: 0.8861 - rmse: 0.1425 - val_loss: 0.1938 - val_r_square: 0.6510 - val_rmse: 0.1938\n",
      "Epoch 33/100\n",
      "1263/1263 - 0s - loss: 0.1472 - r_square: 0.8845 - rmse: 0.1472 - val_loss: 0.2015 - val_r_square: 0.6484 - val_rmse: 0.2015\n",
      "Epoch 34/100\n",
      "1263/1263 - 0s - loss: 0.1440 - r_square: 0.8866 - rmse: 0.1440 - val_loss: 0.2026 - val_r_square: 0.6496 - val_rmse: 0.2026\n",
      "Epoch 35/100\n",
      "1263/1263 - 0s - loss: 0.1402 - r_square: 0.8868 - rmse: 0.1402 - val_loss: 0.2034 - val_r_square: 0.6651 - val_rmse: 0.2034\n",
      "Epoch 36/100\n",
      "1263/1263 - 0s - loss: 0.1372 - r_square: 0.8951 - rmse: 0.1372 - val_loss: 0.2038 - val_r_square: 0.6597 - val_rmse: 0.2038\n",
      "Epoch 37/100\n",
      "1263/1263 - 0s - loss: 0.1470 - r_square: 0.9015 - rmse: 0.1470 - val_loss: 0.1990 - val_r_square: 0.6638 - val_rmse: 0.1990\n",
      "Epoch 38/100\n",
      "1263/1263 - 0s - loss: 0.1393 - r_square: 0.8905 - rmse: 0.1393 - val_loss: 0.1972 - val_r_square: 0.6634 - val_rmse: 0.1972\n",
      "Epoch 39/100\n",
      "1263/1263 - 0s - loss: 0.1440 - r_square: 0.8968 - rmse: 0.1440 - val_loss: 0.2216 - val_r_square: 0.6732 - val_rmse: 0.2216\n",
      "Epoch 40/100\n",
      "1263/1263 - 0s - loss: 0.1405 - r_square: 0.8959 - rmse: 0.1405 - val_loss: 0.2015 - val_r_square: 0.6790 - val_rmse: 0.2015\n",
      "Epoch 41/100\n",
      "1263/1263 - 0s - loss: 0.1351 - r_square: 0.8987 - rmse: 0.1351 - val_loss: 0.1940 - val_r_square: 0.6713 - val_rmse: 0.1940\n",
      "Epoch 42/100\n",
      "1263/1263 - 0s - loss: 0.1388 - r_square: 0.8997 - rmse: 0.1388 - val_loss: 0.2023 - val_r_square: 0.6782 - val_rmse: 0.2023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f8b00f33c8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#early stopping tuning #1\n",
    "from keras.callbacks import EarlyStopping\n",
    "es= EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10,verbose=0, mode='min')\n",
    "model.fit(\n",
    "    X_test_scaled,\n",
    "    y_test_scaled,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    validation_split= .15,\n",
    "    callbacks= [es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1486/1486 [==============================] - 0s 21us/sample - loss: 0.1491 - r_square: 0.8638 - rmse: 0.1491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1491281768334666, 0.86377335, 0.14912818]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test_scaled, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1263 samples, validate on 223 samples\n",
      "Epoch 1/100\n",
      "1263/1263 - 0s - loss: 0.1374 - r_square: 0.9003 - rmse: 0.1374 - val_loss: 0.1968 - val_r_square: 0.6748 - val_rmse: 0.1968\n",
      "Epoch 2/100\n",
      "1263/1263 - 0s - loss: 0.1350 - r_square: 0.8948 - rmse: 0.1350 - val_loss: 0.1882 - val_r_square: 0.6859 - val_rmse: 0.1882\n",
      "Epoch 3/100\n",
      "1263/1263 - 0s - loss: 0.1351 - r_square: 0.9055 - rmse: 0.1351 - val_loss: 0.1986 - val_r_square: 0.6768 - val_rmse: 0.1986\n",
      "Epoch 4/100\n",
      "1263/1263 - 0s - loss: 0.1376 - r_square: 0.9022 - rmse: 0.1376 - val_loss: 0.1898 - val_r_square: 0.6879 - val_rmse: 0.1898\n",
      "Epoch 5/100\n",
      "1263/1263 - 0s - loss: 0.1375 - r_square: 0.9036 - rmse: 0.1375 - val_loss: 0.2024 - val_r_square: 0.6869 - val_rmse: 0.2024\n",
      "Epoch 6/100\n",
      "1263/1263 - 0s - loss: 0.1358 - r_square: 0.9087 - rmse: 0.1358 - val_loss: 0.1878 - val_r_square: 0.6912 - val_rmse: 0.1878\n",
      "Epoch 7/100\n",
      "1263/1263 - 0s - loss: 0.1298 - r_square: 0.9048 - rmse: 0.1298 - val_loss: 0.2033 - val_r_square: 0.6876 - val_rmse: 0.2033\n",
      "Epoch 8/100\n",
      "1263/1263 - 0s - loss: 0.1371 - r_square: 0.9056 - rmse: 0.1371 - val_loss: 0.1907 - val_r_square: 0.6878 - val_rmse: 0.1907\n",
      "Epoch 9/100\n",
      "1263/1263 - 0s - loss: 0.1333 - r_square: 0.9130 - rmse: 0.1333 - val_loss: 0.2011 - val_r_square: 0.6907 - val_rmse: 0.2011\n",
      "Epoch 10/100\n",
      "1263/1263 - 0s - loss: 0.1290 - r_square: 0.9059 - rmse: 0.1290 - val_loss: 0.1902 - val_r_square: 0.6933 - val_rmse: 0.1902\n",
      "Epoch 11/100\n",
      "1263/1263 - 0s - loss: 0.1282 - r_square: 0.9046 - rmse: 0.1282 - val_loss: 0.1894 - val_r_square: 0.7045 - val_rmse: 0.1894\n",
      "Epoch 12/100\n",
      "1263/1263 - 0s - loss: 0.1287 - r_square: 0.9067 - rmse: 0.1287 - val_loss: 0.1923 - val_r_square: 0.6955 - val_rmse: 0.1923\n",
      "Epoch 13/100\n",
      "1263/1263 - 0s - loss: 0.1371 - r_square: 0.9053 - rmse: 0.1371 - val_loss: 0.1941 - val_r_square: 0.7065 - val_rmse: 0.1941\n",
      "Epoch 14/100\n",
      "1263/1263 - 0s - loss: 0.1337 - r_square: 0.9102 - rmse: 0.1337 - val_loss: 0.1894 - val_r_square: 0.7031 - val_rmse: 0.1894\n",
      "Epoch 15/100\n",
      "1263/1263 - 0s - loss: 0.1255 - r_square: 0.9021 - rmse: 0.1255 - val_loss: 0.1892 - val_r_square: 0.7127 - val_rmse: 0.1892\n",
      "Epoch 16/100\n",
      "1263/1263 - 0s - loss: 0.1289 - r_square: 0.9121 - rmse: 0.1289 - val_loss: 0.1830 - val_r_square: 0.7108 - val_rmse: 0.1830\n",
      "Epoch 17/100\n",
      "1263/1263 - 0s - loss: 0.1291 - r_square: 0.9029 - rmse: 0.1291 - val_loss: 0.1833 - val_r_square: 0.7110 - val_rmse: 0.1833\n",
      "Epoch 18/100\n",
      "1263/1263 - 0s - loss: 0.1249 - r_square: 0.9074 - rmse: 0.1249 - val_loss: 0.1824 - val_r_square: 0.7082 - val_rmse: 0.1824\n",
      "Epoch 19/100\n",
      "1263/1263 - 0s - loss: 0.1268 - r_square: 0.8985 - rmse: 0.1268 - val_loss: 0.1816 - val_r_square: 0.7138 - val_rmse: 0.1816\n",
      "Epoch 20/100\n",
      "1263/1263 - 0s - loss: 0.1277 - r_square: 0.9038 - rmse: 0.1277 - val_loss: 0.1834 - val_r_square: 0.7117 - val_rmse: 0.1834\n",
      "Epoch 21/100\n",
      "1263/1263 - 0s - loss: 0.1288 - r_square: 0.8973 - rmse: 0.1288 - val_loss: 0.1861 - val_r_square: 0.7217 - val_rmse: 0.1861\n",
      "Epoch 22/100\n",
      "1263/1263 - 0s - loss: 0.1290 - r_square: 0.9121 - rmse: 0.1290 - val_loss: 0.1899 - val_r_square: 0.7276 - val_rmse: 0.1899\n",
      "Epoch 23/100\n",
      "1263/1263 - 0s - loss: 0.1240 - r_square: 0.9109 - rmse: 0.1240 - val_loss: 0.1915 - val_r_square: 0.7151 - val_rmse: 0.1915\n",
      "Epoch 24/100\n",
      "1263/1263 - 0s - loss: 0.1220 - r_square: 0.9143 - rmse: 0.1220 - val_loss: 0.1820 - val_r_square: 0.7234 - val_rmse: 0.1820\n",
      "Epoch 25/100\n",
      "1263/1263 - 0s - loss: 0.1243 - r_square: 0.9156 - rmse: 0.1243 - val_loss: 0.1898 - val_r_square: 0.7181 - val_rmse: 0.1898\n",
      "Epoch 26/100\n",
      "1263/1263 - 0s - loss: 0.1351 - r_square: 0.9065 - rmse: 0.1351 - val_loss: 0.1958 - val_r_square: 0.7361 - val_rmse: 0.1958\n",
      "Epoch 27/100\n",
      "1263/1263 - 0s - loss: 0.1316 - r_square: 0.9059 - rmse: 0.1316 - val_loss: 0.2159 - val_r_square: 0.7222 - val_rmse: 0.2159\n",
      "Epoch 28/100\n",
      "1263/1263 - 0s - loss: 0.1287 - r_square: 0.9074 - rmse: 0.1287 - val_loss: 0.2055 - val_r_square: 0.7222 - val_rmse: 0.2055\n",
      "Epoch 29/100\n",
      "1263/1263 - 0s - loss: 0.1279 - r_square: 0.9111 - rmse: 0.1279 - val_loss: 0.1866 - val_r_square: 0.7332 - val_rmse: 0.1866\n",
      "Epoch 30/100\n",
      "1263/1263 - 0s - loss: 0.1234 - r_square: 0.9120 - rmse: 0.1234 - val_loss: 0.1883 - val_r_square: 0.7334 - val_rmse: 0.1883\n",
      "Epoch 31/100\n",
      "1263/1263 - 0s - loss: 0.1203 - r_square: 0.9050 - rmse: 0.1203 - val_loss: 0.1786 - val_r_square: 0.7396 - val_rmse: 0.1786\n",
      "Epoch 32/100\n",
      "1263/1263 - 0s - loss: 0.1263 - r_square: 0.9170 - rmse: 0.1263 - val_loss: 0.1769 - val_r_square: 0.7432 - val_rmse: 0.1769\n",
      "Epoch 33/100\n",
      "1263/1263 - 0s - loss: 0.1228 - r_square: 0.9166 - rmse: 0.1228 - val_loss: 0.1864 - val_r_square: 0.7346 - val_rmse: 0.1864\n",
      "Epoch 34/100\n",
      "1263/1263 - 0s - loss: 0.1228 - r_square: 0.9121 - rmse: 0.1228 - val_loss: 0.1763 - val_r_square: 0.7481 - val_rmse: 0.1763\n",
      "Epoch 35/100\n",
      "1263/1263 - 0s - loss: 0.1218 - r_square: 0.9174 - rmse: 0.1218 - val_loss: 0.1922 - val_r_square: 0.7433 - val_rmse: 0.1922\n",
      "Epoch 36/100\n",
      "1263/1263 - 0s - loss: 0.1357 - r_square: 0.9115 - rmse: 0.1357 - val_loss: 0.1814 - val_r_square: 0.7405 - val_rmse: 0.1814\n",
      "Epoch 37/100\n",
      "1263/1263 - 0s - loss: 0.1224 - r_square: 0.9125 - rmse: 0.1224 - val_loss: 0.1817 - val_r_square: 0.7521 - val_rmse: 0.1817\n",
      "Epoch 38/100\n",
      "1263/1263 - 0s - loss: 0.1349 - r_square: 0.9066 - rmse: 0.1349 - val_loss: 0.1896 - val_r_square: 0.7484 - val_rmse: 0.1896\n",
      "Epoch 39/100\n",
      "1263/1263 - 0s - loss: 0.1275 - r_square: 0.9128 - rmse: 0.1275 - val_loss: 0.2038 - val_r_square: 0.7389 - val_rmse: 0.2038\n",
      "Epoch 40/100\n",
      "1263/1263 - 0s - loss: 0.1243 - r_square: 0.9138 - rmse: 0.1243 - val_loss: 0.1745 - val_r_square: 0.7500 - val_rmse: 0.1745\n",
      "Epoch 41/100\n",
      "1263/1263 - 0s - loss: 0.1205 - r_square: 0.9173 - rmse: 0.1205 - val_loss: 0.1859 - val_r_square: 0.7518 - val_rmse: 0.1859\n",
      "Epoch 42/100\n",
      "1263/1263 - 0s - loss: 0.1312 - r_square: 0.9079 - rmse: 0.1312 - val_loss: 0.1798 - val_r_square: 0.7611 - val_rmse: 0.1798\n",
      "Epoch 43/100\n",
      "1263/1263 - 0s - loss: 0.1241 - r_square: 0.9133 - rmse: 0.1241 - val_loss: 0.1793 - val_r_square: 0.7570 - val_rmse: 0.1793\n",
      "Epoch 44/100\n",
      "1263/1263 - 0s - loss: 0.1235 - r_square: 0.9106 - rmse: 0.1235 - val_loss: 0.1912 - val_r_square: 0.7615 - val_rmse: 0.1912\n",
      "Epoch 45/100\n",
      "1263/1263 - 0s - loss: 0.1246 - r_square: 0.9133 - rmse: 0.1246 - val_loss: 0.1866 - val_r_square: 0.7612 - val_rmse: 0.1866\n",
      "Epoch 46/100\n",
      "1263/1263 - 0s - loss: 0.1201 - r_square: 0.9173 - rmse: 0.1201 - val_loss: 0.1755 - val_r_square: 0.7673 - val_rmse: 0.1755\n",
      "Epoch 47/100\n",
      "1263/1263 - 0s - loss: 0.1175 - r_square: 0.9171 - rmse: 0.1175 - val_loss: 0.1858 - val_r_square: 0.7559 - val_rmse: 0.1858\n",
      "Epoch 48/100\n",
      "1263/1263 - 0s - loss: 0.1216 - r_square: 0.9180 - rmse: 0.1216 - val_loss: 0.2000 - val_r_square: 0.7662 - val_rmse: 0.2000\n",
      "Epoch 49/100\n",
      "1263/1263 - 0s - loss: 0.1235 - r_square: 0.9157 - rmse: 0.1235 - val_loss: 0.1786 - val_r_square: 0.7754 - val_rmse: 0.1786\n",
      "Epoch 50/100\n",
      "1263/1263 - 0s - loss: 0.1219 - r_square: 0.9189 - rmse: 0.1219 - val_loss: 0.1821 - val_r_square: 0.7772 - val_rmse: 0.1821\n",
      "Epoch 51/100\n",
      "1263/1263 - 0s - loss: 0.1196 - r_square: 0.9193 - rmse: 0.1196 - val_loss: 0.2023 - val_r_square: 0.7697 - val_rmse: 0.2023\n",
      "Epoch 52/100\n",
      "1263/1263 - 0s - loss: 0.1200 - r_square: 0.9202 - rmse: 0.1200 - val_loss: 0.1836 - val_r_square: 0.7786 - val_rmse: 0.1836\n",
      "Epoch 53/100\n",
      "1263/1263 - 0s - loss: 0.1182 - r_square: 0.9212 - rmse: 0.1182 - val_loss: 0.1870 - val_r_square: 0.7718 - val_rmse: 0.1870\n",
      "Epoch 54/100\n",
      "1263/1263 - 0s - loss: 0.1176 - r_square: 0.9259 - rmse: 0.1176 - val_loss: 0.1812 - val_r_square: 0.7809 - val_rmse: 0.1812\n",
      "Epoch 55/100\n",
      "1263/1263 - 0s - loss: 0.1212 - r_square: 0.9180 - rmse: 0.1212 - val_loss: 0.1905 - val_r_square: 0.7732 - val_rmse: 0.1905\n",
      "Epoch 56/100\n",
      "1263/1263 - 0s - loss: 0.1184 - r_square: 0.9197 - rmse: 0.1184 - val_loss: 0.1755 - val_r_square: 0.7853 - val_rmse: 0.1755\n",
      "Epoch 57/100\n",
      "1263/1263 - 0s - loss: 0.1161 - r_square: 0.9219 - rmse: 0.1161 - val_loss: 0.1760 - val_r_square: 0.7860 - val_rmse: 0.1760\n",
      "Epoch 58/100\n",
      "1263/1263 - 0s - loss: 0.1222 - r_square: 0.9201 - rmse: 0.1222 - val_loss: 0.1984 - val_r_square: 0.7675 - val_rmse: 0.1984\n",
      "Epoch 59/100\n",
      "1263/1263 - 0s - loss: 0.1236 - r_square: 0.9196 - rmse: 0.1236 - val_loss: 0.1854 - val_r_square: 0.7886 - val_rmse: 0.1854\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1263/1263 - 0s - loss: 0.1186 - r_square: 0.9251 - rmse: 0.1186 - val_loss: 0.1772 - val_r_square: 0.7918 - val_rmse: 0.1772\n",
      "Epoch 61/100\n",
      "1263/1263 - 0s - loss: 0.1158 - r_square: 0.9237 - rmse: 0.1158 - val_loss: 0.1717 - val_r_square: 0.7905 - val_rmse: 0.1717\n",
      "Epoch 62/100\n",
      "1263/1263 - 0s - loss: 0.1156 - r_square: 0.9227 - rmse: 0.1156 - val_loss: 0.1730 - val_r_square: 0.7928 - val_rmse: 0.1730\n",
      "Epoch 63/100\n",
      "1263/1263 - 0s - loss: 0.1170 - r_square: 0.9204 - rmse: 0.1170 - val_loss: 0.1848 - val_r_square: 0.7883 - val_rmse: 0.1848\n",
      "Epoch 64/100\n",
      "1263/1263 - 0s - loss: 0.1164 - r_square: 0.9165 - rmse: 0.1164 - val_loss: 0.1755 - val_r_square: 0.7901 - val_rmse: 0.1755\n",
      "Epoch 65/100\n",
      "1263/1263 - 0s - loss: 0.1138 - r_square: 0.9223 - rmse: 0.1138 - val_loss: 0.1713 - val_r_square: 0.7948 - val_rmse: 0.1713\n",
      "Epoch 66/100\n",
      "1263/1263 - 0s - loss: 0.1157 - r_square: 0.9289 - rmse: 0.1157 - val_loss: 0.1777 - val_r_square: 0.7879 - val_rmse: 0.1777\n",
      "Epoch 67/100\n",
      "1263/1263 - 0s - loss: 0.1191 - r_square: 0.9276 - rmse: 0.1191 - val_loss: 0.1778 - val_r_square: 0.7897 - val_rmse: 0.1778\n",
      "Epoch 68/100\n",
      "1263/1263 - 0s - loss: 0.1129 - r_square: 0.9300 - rmse: 0.1129 - val_loss: 0.1774 - val_r_square: 0.7881 - val_rmse: 0.1774\n",
      "Epoch 69/100\n",
      "1263/1263 - 0s - loss: 0.1162 - r_square: 0.9221 - rmse: 0.1162 - val_loss: 0.1776 - val_r_square: 0.7975 - val_rmse: 0.1776\n",
      "Epoch 70/100\n",
      "1263/1263 - 0s - loss: 0.1167 - r_square: 0.9254 - rmse: 0.1167 - val_loss: 0.1724 - val_r_square: 0.8008 - val_rmse: 0.1724\n",
      "Epoch 71/100\n",
      "1263/1263 - 0s - loss: 0.1154 - r_square: 0.9228 - rmse: 0.1154 - val_loss: 0.1743 - val_r_square: 0.7955 - val_rmse: 0.1743\n",
      "Epoch 72/100\n",
      "1263/1263 - 0s - loss: 0.1132 - r_square: 0.9246 - rmse: 0.1132 - val_loss: 0.1789 - val_r_square: 0.7950 - val_rmse: 0.1789\n",
      "Epoch 73/100\n",
      "1263/1263 - 0s - loss: 0.1170 - r_square: 0.9235 - rmse: 0.1170 - val_loss: 0.1812 - val_r_square: 0.7996 - val_rmse: 0.1812\n",
      "Epoch 74/100\n",
      "1263/1263 - 0s - loss: 0.1149 - r_square: 0.9237 - rmse: 0.1149 - val_loss: 0.1824 - val_r_square: 0.7896 - val_rmse: 0.1824\n",
      "Epoch 75/100\n",
      "1263/1263 - 0s - loss: 0.1166 - r_square: 0.9261 - rmse: 0.1166 - val_loss: 0.1656 - val_r_square: 0.8053 - val_rmse: 0.1656\n",
      "Epoch 76/100\n",
      "1263/1263 - 0s - loss: 0.1172 - r_square: 0.9274 - rmse: 0.1172 - val_loss: 0.1801 - val_r_square: 0.8025 - val_rmse: 0.1801\n",
      "Epoch 77/100\n",
      "1263/1263 - 0s - loss: 0.1122 - r_square: 0.9293 - rmse: 0.1122 - val_loss: 0.1697 - val_r_square: 0.8046 - val_rmse: 0.1697\n",
      "Epoch 78/100\n",
      "1263/1263 - 0s - loss: 0.1095 - r_square: 0.9279 - rmse: 0.1095 - val_loss: 0.1735 - val_r_square: 0.8031 - val_rmse: 0.1735\n",
      "Epoch 79/100\n",
      "1263/1263 - 0s - loss: 0.1123 - r_square: 0.9272 - rmse: 0.1123 - val_loss: 0.1685 - val_r_square: 0.8147 - val_rmse: 0.1685\n",
      "Epoch 80/100\n",
      "1263/1263 - 0s - loss: 0.1168 - r_square: 0.9270 - rmse: 0.1168 - val_loss: 0.1789 - val_r_square: 0.8025 - val_rmse: 0.1789\n",
      "Epoch 81/100\n",
      "1263/1263 - 0s - loss: 0.1142 - r_square: 0.9275 - rmse: 0.1142 - val_loss: 0.1699 - val_r_square: 0.8119 - val_rmse: 0.1699\n",
      "Epoch 82/100\n",
      "1263/1263 - 0s - loss: 0.1101 - r_square: 0.9266 - rmse: 0.1101 - val_loss: 0.1770 - val_r_square: 0.8057 - val_rmse: 0.1770\n",
      "Epoch 83/100\n",
      "1263/1263 - 0s - loss: 0.1141 - r_square: 0.9266 - rmse: 0.1141 - val_loss: 0.1677 - val_r_square: 0.8076 - val_rmse: 0.1677\n",
      "Epoch 84/100\n",
      "1263/1263 - 0s - loss: 0.1122 - r_square: 0.9292 - rmse: 0.1122 - val_loss: 0.1673 - val_r_square: 0.8128 - val_rmse: 0.1673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f8b16dacc8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#early stopping tuning #2\n",
    "from keras.callbacks import EarlyStopping\n",
    "es= EarlyStopping(monitor='val_r_square', min_delta=0.000001, patience=5,verbose=0, mode='max')\n",
    "model.fit(\n",
    "    X_test_scaled,\n",
    "    y_test_scaled,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    validation_split= .15,\n",
    "    callbacks= [es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1486/1486 [==============================] - 0s 33us/sample - loss: 0.1162 - r_square: 0.9078 - rmse: 0.1162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11616962148355443, 0.90783346, 0.11616962]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test_scaled, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperas Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into test and train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# # Create a StandardScater model and fit it to the training data\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "y_scaler = StandardScaler().fit(y_train)\n",
    "# # Transform the training and testing data using the X_scaler and y_scaler models\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "y_train_scaled = y_scaler.transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to scale data for create model function\n",
    "def data():\n",
    "    #read in data for analysis \n",
    "    all_wells = pd.read_csv('Cleaned_Data/well_cleaned.csv')\n",
    "    X1= all_wells[[\"AVG_CHOKE_SIZE_P\",\"AVG_WHP_P\",\"AVG_WHT_P\",\"BORE_OIL_VOL\",\"BORE_GAS_VOL\", \"BORE_WAT_VOL\"]]\n",
    "    y1= all_wells[\"AVG_DOWNHOLE_PRESSURE\"].values.reshape(-1, 1)\n",
    "    #split into test and train data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=42)\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    # # Create a StandardScater model and fit it to the training data\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "    y_scaler = StandardScaler().fit(y_train)\n",
    "    # # Transform the training and testing data using the X_scaler and y_scaler models\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "    y_train_scaled = y_scaler.transform(y_train)\n",
    "    y_test_scaled = y_scaler.transform(y_test)\n",
    "    \n",
    "    x_train = X_train_scaled.reshape(-1,6)\n",
    "    x_test = X_test_scaled.reshape(-1,6)\n",
    "    y_train = y_train_scaled.reshape(-1,1)\n",
    "    y_test = y_test_scaled.reshape(-1,1)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics\n",
    "# CREDIT: https://github.com/keras-team/keras/issues/7947\n",
    "# root mean squared error (rmse) for regression (only for Keras tensors)\n",
    "def rmse(y_true, y_pred):\n",
    "    from keras import backend\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "# mean squared error (mse) for regression  (only for Keras tensors)\n",
    "def mse(y_true, y_pred):\n",
    "    from keras import backend\n",
    "    return backend.mean(backend.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "# coefficient of determination (R^2) for regression  (only for Keras tensors)\n",
    "def r_square(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import losses\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import print_summary\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dense': hp.choice('Dense', [50,200,500]),\n",
      "        'Dropout': hp.uniform('Dropout', 0,1),\n",
      "        'Dense_1': hp.choice('Dense_1', [50,200,500]),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0,1),\n",
      "        'batch_size': hp.choice('batch_size', [64,128]),\n",
      "        'epochs': hp.choice('epochs', [50,100,150]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: #read in data for analysis \n",
      "   3: all_wells = pd.read_csv('Cleaned_Data/well_cleaned.csv')\n",
      "   4: X1= all_wells[[\"AVG_CHOKE_SIZE_P\",\"AVG_WHP_P\",\"AVG_WHT_P\",\"BORE_OIL_VOL\",\"BORE_GAS_VOL\", \"BORE_WAT_VOL\"]]\n",
      "   5: y1= all_wells[\"AVG_DOWNHOLE_PRESSURE\"].values.reshape(-1, 1)\n",
      "   6: #split into test and train data\n",
      "   7: from sklearn.model_selection import train_test_split\n",
      "   8: X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=42)\n",
      "   9: from sklearn.preprocessing import StandardScaler\n",
      "  10: # # Create a StandardScater model and fit it to the training data\n",
      "  11: X_scaler = StandardScaler().fit(X_train)\n",
      "  12: y_scaler = StandardScaler().fit(y_train)\n",
      "  13: # # Transform the training and testing data using the X_scaler and y_scaler models\n",
      "  14: X_train_scaled = X_scaler.transform(X_train)\n",
      "  15: X_test_scaled = X_scaler.transform(X_test)\n",
      "  16: y_train_scaled = y_scaler.transform(y_train)\n",
      "  17: y_test_scaled = y_scaler.transform(y_test)\n",
      "  18: \n",
      "  19: x_train = X_train_scaled.reshape(-1,6)\n",
      "  20: x_test = X_test_scaled.reshape(-1,6)\n",
      "  21: y_train = y_train_scaled.reshape(-1,1)\n",
      "  22: y_test = y_test_scaled.reshape(-1,1)\n",
      "  23: \n",
      "  24: \n",
      "  25: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     print(x_train.shape)\n",
      "   4:     model= Sequential() \n",
      "   5:     model.add(Dense(space['Dense'], input_dim=x_train.shape[1], activation= 'relu'))\n",
      "   6:     model.add(Dropout(space['Dropout']))\n",
      "   7:     model.add(Dense(space['Dense_1'],activation= 'relu'))\n",
      "   8:     #model.add(Activation('relu'))\n",
      "   9:     model.add(Dropout(space['Dropout_1']))\n",
      "  10:     model.add(Dense(1, activation= 'linear'))\n",
      "  11: \n",
      "  12:     \n",
      "  13: ################################################\n",
      "  14: # CREDIT: https://github.com/keras-team/keras/issues/7947\n",
      "  15:     def rmse(y_true, y_pred):\n",
      "  16:         return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
      "  17: # mean squared error (mse) for regression  (only for Keras tensors)\n",
      "  18:     def mse(y_true, y_pred):\n",
      "  19:         return backend.mean(backend.square(y_pred - y_true), axis=-1)\n",
      "  20:     def r_square(y_true, y_pred):\n",
      "  21:         SS_res =  K.sum(K.square(y_true - y_pred)) \n",
      "  22:         SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
      "  23:         return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
      "  24: #############################################\n",
      "  25: \n",
      "  26:     model.compile(loss='mean_absolute_error', optimizer= 'adam', metrics=[r_square, rmse])\n",
      "  27:     print_summary(model, line_length=None, positions=None, print_fn=None)\n",
      "  28:     result= model.fit(x_train, y_train,\n",
      "  29:                       batch_size=space['batch_size'],\n",
      "  30:                       epochs=space['epochs'],\n",
      "  31:                       verbose=2,\n",
      "  32:                       validation_split =0.15)\n",
      "  33:     validation_acc= np.min(result.history['val_loss'])\n",
      "  34:     print('Lowest Validation Loss:', validation_acc)\n",
      "  35:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}   \n",
      "  36: \n",
      "(4457, 6)                                                                                                              \n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]WARNING:tensorflow:Large dropout rate: 0.73717 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.651797 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_1\"                                                                                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "dense_1 (Dense)              (None, 200)               1400                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 200)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 200)               40200                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dropout_2 (Dropout)          (None, 200)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_3 (Dense)              (None, 1)                 201                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 41,801                                                                                                   \n",
      "Trainable params: 41,801                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "Train on 3788 samples, validate on 669 samples                                                                         \n",
      "Epoch 1/150                                                                                                            \n",
      " - 0s - loss: 0.7040 - r_square: 0.1228 - rmse: 0.7040 - val_loss: 0.4904 - val_r_square: 0.5927 - val_rmse: 0.4904    \n",
      "\n",
      "Epoch 2/150                                                                                                            \n",
      " - 0s - loss: 0.5669 - r_square: 0.4141 - rmse: 0.5669 - val_loss: 0.4110 - val_r_square: 0.6869 - val_rmse: 0.4110    \n",
      "\n",
      "Epoch 3/150                                                                                                            \n",
      " - 0s - loss: 0.5130 - r_square: 0.4994 - rmse: 0.5130 - val_loss: 0.3980 - val_r_square: 0.7138 - val_rmse: 0.3980    \n",
      "\n",
      "Epoch 4/150                                                                                                            \n",
      " - 0s - loss: 0.4719 - r_square: 0.5609 - rmse: 0.4719 - val_loss: 0.3758 - val_r_square: 0.7357 - val_rmse: 0.3758    \n",
      "\n",
      "Epoch 5/150                                                                                                            \n",
      " - 0s - loss: 0.4523 - r_square: 0.5772 - rmse: 0.4523 - val_loss: 0.3590 - val_r_square: 0.7501 - val_rmse: 0.3590    \n",
      "\n",
      "Epoch 6/150                                                                                                            \n",
      " - 0s - loss: 0.4296 - r_square: 0.6052 - rmse: 0.4296 - val_loss: 0.3585 - val_r_square: 0.7531 - val_rmse: 0.3585    \n",
      "\n",
      "Epoch 7/150                                                                                                            \n",
      " - 0s - loss: 0.4137 - r_square: 0.6318 - rmse: 0.4137 - val_loss: 0.3581 - val_r_square: 0.7575 - val_rmse: 0.3581    \n",
      "\n",
      "Epoch 8/150                                                                                                            \n",
      " - 0s - loss: 0.4114 - r_square: 0.6441 - rmse: 0.4114 - val_loss: 0.3738 - val_r_square: 0.7413 - val_rmse: 0.3738    \n",
      "\n",
      "Epoch 9/150                                                                                                            \n",
      " - 0s - loss: 0.3949 - r_square: 0.6583 - rmse: 0.3949 - val_loss: 0.3491 - val_r_square: 0.7661 - val_rmse: 0.3491    \n",
      "\n",
      "Epoch 10/150                                                                                                           \n",
      " - 0s - loss: 0.3905 - r_square: 0.6655 - rmse: 0.3905 - val_loss: 0.3298 - val_r_square: 0.7798 - val_rmse: 0.3298    \n",
      "\n",
      "Epoch 11/150                                                                                                           \n",
      " - 0s - loss: 0.3838 - r_square: 0.6721 - rmse: 0.3838 - val_loss: 0.3391 - val_r_square: 0.7755 - val_rmse: 0.3391    \n",
      "\n",
      "Epoch 12/150                                                                                                           \n",
      " - 0s - loss: 0.3696 - r_square: 0.6830 - rmse: 0.3696 - val_loss: 0.3093 - val_r_square: 0.7895 - val_rmse: 0.3093    \n",
      "\n",
      "Epoch 13/150                                                                                                           \n",
      " - 0s - loss: 0.3779 - r_square: 0.6660 - rmse: 0.3779 - val_loss: 0.3118 - val_r_square: 0.7882 - val_rmse: 0.3118    \n",
      "\n",
      "Epoch 14/150                                                                                                           \n",
      " - 0s - loss: 0.3675 - r_square: 0.6869 - rmse: 0.3675 - val_loss: 0.3116 - val_r_square: 0.7916 - val_rmse: 0.3116    \n",
      "\n",
      "Epoch 15/150                                                                                                           \n",
      " - 0s - loss: 0.3587 - r_square: 0.6893 - rmse: 0.3587 - val_loss: 0.3016 - val_r_square: 0.7950 - val_rmse: 0.3016    \n",
      "\n",
      "Epoch 16/150                                                                                                           \n",
      " - 0s - loss: 0.3538 - r_square: 0.6997 - rmse: 0.3538 - val_loss: 0.2977 - val_r_square: 0.8035 - val_rmse: 0.2977    \n",
      "\n",
      "Epoch 17/150                                                                                                           \n",
      " - 0s - loss: 0.3500 - r_square: 0.7035 - rmse: 0.3500 - val_loss: 0.2903 - val_r_square: 0.8073 - val_rmse: 0.2903    \n",
      "\n",
      "Epoch 18/150                                                                                                           \n",
      " - 0s - loss: 0.3407 - r_square: 0.7125 - rmse: 0.3407 - val_loss: 0.2836 - val_r_square: 0.8098 - val_rmse: 0.2836    \n",
      "\n",
      "Epoch 19/150                                                                                                           \n",
      " - 0s - loss: 0.3390 - r_square: 0.7227 - rmse: 0.3390 - val_loss: 0.2798 - val_r_square: 0.8152 - val_rmse: 0.2798    \n",
      "\n",
      "Epoch 20/150                                                                                                           \n",
      " - 0s - loss: 0.3240 - r_square: 0.7399 - rmse: 0.3240 - val_loss: 0.2779 - val_r_square: 0.8205 - val_rmse: 0.2779    \n",
      "\n",
      "Epoch 21/150                                                                                                           \n",
      " - 0s - loss: 0.3230 - r_square: 0.7401 - rmse: 0.3230 - val_loss: 0.2708 - val_r_square: 0.8216 - val_rmse: 0.2708    \n",
      "\n",
      "Epoch 22/150                                                                                                           \n",
      " - 0s - loss: 0.3241 - r_square: 0.7313 - rmse: 0.3241 - val_loss: 0.2764 - val_r_square: 0.8181 - val_rmse: 0.2764    \n",
      "\n",
      "Epoch 23/150                                                                                                           \n",
      " - 0s - loss: 0.3184 - r_square: 0.7410 - rmse: 0.3184 - val_loss: 0.2873 - val_r_square: 0.8149 - val_rmse: 0.2873    \n",
      "\n",
      "Epoch 24/150                                                                                                           \n",
      " - 0s - loss: 0.3188 - r_square: 0.7409 - rmse: 0.3188 - val_loss: 0.2744 - val_r_square: 0.8234 - val_rmse: 0.2744    \n",
      "\n",
      "Epoch 25/150                                                                                                           \n",
      " - 0s - loss: 0.3212 - r_square: 0.7397 - rmse: 0.3212 - val_loss: 0.2694 - val_r_square: 0.8234 - val_rmse: 0.2694    \n",
      "\n",
      "Epoch 26/150                                                                                                           \n",
      " - 0s - loss: 0.3146 - r_square: 0.7451 - rmse: 0.3146 - val_loss: 0.2598 - val_r_square: 0.8321 - val_rmse: 0.2598    \n",
      "\n",
      "Epoch 27/150                                                                                                           \n",
      " - 0s - loss: 0.3063 - r_square: 0.7523 - rmse: 0.3063 - val_loss: 0.2568 - val_r_square: 0.8326 - val_rmse: 0.2568    \n",
      "\n",
      "Epoch 28/150                                                                                                           \n",
      " - 0s - loss: 0.3068 - r_square: 0.7521 - rmse: 0.3068 - val_loss: 0.2799 - val_r_square: 0.8180 - val_rmse: 0.2799    \n",
      "\n",
      "Epoch 29/150                                                                                                           \n",
      " - 0s - loss: 0.3091 - r_square: 0.7541 - rmse: 0.3091 - val_loss: 0.2790 - val_r_square: 0.8192 - val_rmse: 0.2790    \n",
      "\n",
      "Epoch 30/150                                                                                                           \n",
      " - 0s - loss: 0.3074 - r_square: 0.7500 - rmse: 0.3074 - val_loss: 0.2858 - val_r_square: 0.8143 - val_rmse: 0.2858    \n",
      "\n",
      "Epoch 31/150                                                                                                           \n",
      " - 0s - loss: 0.3003 - r_square: 0.7690 - rmse: 0.3003 - val_loss: 0.2687 - val_r_square: 0.8254 - val_rmse: 0.2687    \n",
      "\n",
      "Epoch 32/150                                                                                                           \n",
      " - 0s - loss: 0.3042 - r_square: 0.7586 - rmse: 0.3042 - val_loss: 0.2610 - val_r_square: 0.8293 - val_rmse: 0.2610    \n",
      "\n",
      "Epoch 33/150                                                                                                           \n",
      " - 0s - loss: 0.2985 - r_square: 0.7735 - rmse: 0.2985 - val_loss: 0.2695 - val_r_square: 0.8297 - val_rmse: 0.2695    \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/150                                                                                                           \n",
      " - 0s - loss: 0.2989 - r_square: 0.7681 - rmse: 0.2989 - val_loss: 0.2553 - val_r_square: 0.8344 - val_rmse: 0.2553    \n",
      "\n",
      "Epoch 35/150                                                                                                           \n",
      " - 0s - loss: 0.2913 - r_square: 0.7669 - rmse: 0.2913 - val_loss: 0.2709 - val_r_square: 0.8228 - val_rmse: 0.2709    \n",
      "\n",
      "Epoch 36/150                                                                                                           \n",
      " - 0s - loss: 0.2928 - r_square: 0.7678 - rmse: 0.2928 - val_loss: 0.2463 - val_r_square: 0.8423 - val_rmse: 0.2463    \n",
      "\n",
      "Epoch 37/150                                                                                                           \n",
      " - 0s - loss: 0.2914 - r_square: 0.7781 - rmse: 0.2914 - val_loss: 0.2550 - val_r_square: 0.8378 - val_rmse: 0.2550    \n",
      "\n",
      "Epoch 38/150                                                                                                           \n",
      " - 0s - loss: 0.2935 - r_square: 0.7743 - rmse: 0.2935 - val_loss: 0.2592 - val_r_square: 0.8339 - val_rmse: 0.2592    \n",
      "\n",
      "Epoch 39/150                                                                                                           \n",
      " - 0s - loss: 0.2923 - r_square: 0.7740 - rmse: 0.2923 - val_loss: 0.2512 - val_r_square: 0.8373 - val_rmse: 0.2512    \n",
      "\n",
      "Epoch 40/150                                                                                                           \n",
      " - 0s - loss: 0.2927 - r_square: 0.7739 - rmse: 0.2927 - val_loss: 0.2701 - val_r_square: 0.8245 - val_rmse: 0.2701    \n",
      "\n",
      "Epoch 41/150                                                                                                           \n",
      " - 0s - loss: 0.2897 - r_square: 0.7701 - rmse: 0.2897 - val_loss: 0.2526 - val_r_square: 0.8387 - val_rmse: 0.2526    \n",
      "\n",
      "Epoch 42/150                                                                                                           \n",
      " - 0s - loss: 0.2876 - r_square: 0.7733 - rmse: 0.2876 - val_loss: 0.2648 - val_r_square: 0.8267 - val_rmse: 0.2648    \n",
      "\n",
      "Epoch 43/150                                                                                                           \n",
      " - 0s - loss: 0.2822 - r_square: 0.7783 - rmse: 0.2822 - val_loss: 0.2550 - val_r_square: 0.8343 - val_rmse: 0.2550    \n",
      "\n",
      "Epoch 44/150                                                                                                           \n",
      " - 0s - loss: 0.2833 - r_square: 0.7768 - rmse: 0.2833 - val_loss: 0.2530 - val_r_square: 0.8406 - val_rmse: 0.2530    \n",
      "\n",
      "Epoch 45/150                                                                                                           \n",
      " - 0s - loss: 0.2798 - r_square: 0.7942 - rmse: 0.2798 - val_loss: 0.2602 - val_r_square: 0.8354 - val_rmse: 0.2602    \n",
      "\n",
      "Epoch 46/150                                                                                                           \n",
      " - 0s - loss: 0.2786 - r_square: 0.7883 - rmse: 0.2786 - val_loss: 0.2585 - val_r_square: 0.8357 - val_rmse: 0.2585    \n",
      "\n",
      "Epoch 47/150                                                                                                           \n",
      " - 0s - loss: 0.2863 - r_square: 0.7740 - rmse: 0.2863 - val_loss: 0.2541 - val_r_square: 0.8382 - val_rmse: 0.2541    \n",
      "\n",
      "Epoch 48/150                                                                                                           \n",
      " - 0s - loss: 0.2820 - r_square: 0.7839 - rmse: 0.2820 - val_loss: 0.2360 - val_r_square: 0.8511 - val_rmse: 0.2360    \n",
      "\n",
      "Epoch 49/150                                                                                                           \n",
      " - 0s - loss: 0.2862 - r_square: 0.7810 - rmse: 0.2862 - val_loss: 0.2600 - val_r_square: 0.8355 - val_rmse: 0.2600    \n",
      "\n",
      "Epoch 50/150                                                                                                           \n",
      " - 0s - loss: 0.2831 - r_square: 0.7825 - rmse: 0.2831 - val_loss: 0.2481 - val_r_square: 0.8393 - val_rmse: 0.2481    \n",
      "\n",
      "Epoch 51/150                                                                                                           \n",
      " - 0s - loss: 0.2803 - r_square: 0.7817 - rmse: 0.2803 - val_loss: 0.2562 - val_r_square: 0.8327 - val_rmse: 0.2562    \n",
      "\n",
      "Epoch 52/150                                                                                                           \n",
      " - 0s - loss: 0.2798 - r_square: 0.7812 - rmse: 0.2798 - val_loss: 0.2396 - val_r_square: 0.8476 - val_rmse: 0.2396    \n",
      "\n",
      "Epoch 53/150                                                                                                           \n",
      " - 0s - loss: 0.2839 - r_square: 0.7864 - rmse: 0.2839 - val_loss: 0.2424 - val_r_square: 0.8437 - val_rmse: 0.2424    \n",
      "\n",
      "Epoch 54/150                                                                                                           \n",
      " - 0s - loss: 0.2805 - r_square: 0.7811 - rmse: 0.2805 - val_loss: 0.2410 - val_r_square: 0.8483 - val_rmse: 0.2410    \n",
      "\n",
      "Epoch 55/150                                                                                                           \n",
      " - 0s - loss: 0.2800 - r_square: 0.7842 - rmse: 0.2800 - val_loss: 0.2452 - val_r_square: 0.8457 - val_rmse: 0.2452    \n",
      "\n",
      "Epoch 56/150                                                                                                           \n",
      " - 0s - loss: 0.2723 - r_square: 0.7947 - rmse: 0.2723 - val_loss: 0.2538 - val_r_square: 0.8388 - val_rmse: 0.2538    \n",
      "\n",
      "Epoch 57/150                                                                                                           \n",
      " - 0s - loss: 0.2760 - r_square: 0.7851 - rmse: 0.2760 - val_loss: 0.2375 - val_r_square: 0.8503 - val_rmse: 0.2375    \n",
      "\n",
      "Epoch 58/150                                                                                                           \n",
      " - 0s - loss: 0.2768 - r_square: 0.7924 - rmse: 0.2768 - val_loss: 0.2511 - val_r_square: 0.8390 - val_rmse: 0.2511    \n",
      "\n",
      "Epoch 59/150                                                                                                           \n",
      " - 0s - loss: 0.2752 - r_square: 0.7905 - rmse: 0.2752 - val_loss: 0.2559 - val_r_square: 0.8357 - val_rmse: 0.2559    \n",
      "\n",
      "Epoch 60/150                                                                                                           \n",
      " - 0s - loss: 0.2736 - r_square: 0.7935 - rmse: 0.2736 - val_loss: 0.2524 - val_r_square: 0.8397 - val_rmse: 0.2524    \n",
      "\n",
      "Epoch 61/150                                                                                                           \n",
      " - 0s - loss: 0.2727 - r_square: 0.7852 - rmse: 0.2727 - val_loss: 0.2493 - val_r_square: 0.8417 - val_rmse: 0.2493    \n",
      "\n",
      "Epoch 62/150                                                                                                           \n",
      " - 0s - loss: 0.2733 - r_square: 0.8026 - rmse: 0.2733 - val_loss: 0.2437 - val_r_square: 0.8454 - val_rmse: 0.2437    \n",
      "\n",
      "Epoch 63/150                                                                                                           \n",
      " - 0s - loss: 0.2778 - r_square: 0.7791 - rmse: 0.2778 - val_loss: 0.2522 - val_r_square: 0.8397 - val_rmse: 0.2522    \n",
      "\n",
      "Epoch 64/150                                                                                                           \n",
      " - 0s - loss: 0.2688 - r_square: 0.8033 - rmse: 0.2688 - val_loss: 0.2438 - val_r_square: 0.8442 - val_rmse: 0.2438    \n",
      "\n",
      "Epoch 65/150                                                                                                           \n",
      " - 0s - loss: 0.2715 - r_square: 0.8036 - rmse: 0.2715 - val_loss: 0.2463 - val_r_square: 0.8426 - val_rmse: 0.2463    \n",
      "\n",
      "Epoch 66/150                                                                                                           \n",
      " - 0s - loss: 0.2709 - r_square: 0.7995 - rmse: 0.2709 - val_loss: 0.2439 - val_r_square: 0.8486 - val_rmse: 0.2439    \n",
      "\n",
      "Epoch 67/150                                                                                                           \n",
      " - 0s - loss: 0.2683 - r_square: 0.7895 - rmse: 0.2683 - val_loss: 0.2620 - val_r_square: 0.8364 - val_rmse: 0.2620    \n",
      "\n",
      "Epoch 68/150                                                                                                           \n",
      " - 0s - loss: 0.2728 - r_square: 0.7874 - rmse: 0.2728 - val_loss: 0.2579 - val_r_square: 0.8357 - val_rmse: 0.2579    \n",
      "\n",
      "Epoch 69/150                                                                                                           \n",
      " - 0s - loss: 0.2719 - r_square: 0.7857 - rmse: 0.2719 - val_loss: 0.2413 - val_r_square: 0.8487 - val_rmse: 0.2413    \n",
      "\n",
      "Epoch 70/150                                                                                                           \n",
      " - 0s - loss: 0.2743 - r_square: 0.7830 - rmse: 0.2743 - val_loss: 0.2506 - val_r_square: 0.8414 - val_rmse: 0.2506    \n",
      "\n",
      "Epoch 71/150                                                                                                           \n",
      " - 0s - loss: 0.2722 - r_square: 0.7988 - rmse: 0.2722 - val_loss: 0.2503 - val_r_square: 0.8421 - val_rmse: 0.2503    \n",
      "\n",
      "Epoch 72/150                                                                                                           \n",
      " - 0s - loss: 0.2770 - r_square: 0.7861 - rmse: 0.2770 - val_loss: 0.2544 - val_r_square: 0.8438 - val_rmse: 0.2544    \n",
      "\n",
      "Epoch 73/150                                                                                                           \n",
      " - 0s - loss: 0.2700 - r_square: 0.7909 - rmse: 0.2700 - val_loss: 0.2421 - val_r_square: 0.8478 - val_rmse: 0.2421    \n",
      "\n",
      "Epoch 74/150                                                                                                           \n",
      " - 0s - loss: 0.2741 - r_square: 0.7943 - rmse: 0.2741 - val_loss: 0.2524 - val_r_square: 0.8420 - val_rmse: 0.2524    \n",
      "\n",
      "Epoch 75/150                                                                                                           \n",
      " - 0s - loss: 0.2702 - r_square: 0.7983 - rmse: 0.2702 - val_loss: 0.2327 - val_r_square: 0.8523 - val_rmse: 0.2327    \n",
      "\n",
      "Epoch 76/150                                                                                                           \n",
      " - 0s - loss: 0.2683 - r_square: 0.7903 - rmse: 0.2683 - val_loss: 0.2425 - val_r_square: 0.8461 - val_rmse: 0.2425    \n",
      "\n",
      "Epoch 77/150                                                                                                           \n",
      " - 0s - loss: 0.2692 - r_square: 0.8012 - rmse: 0.2692 - val_loss: 0.2427 - val_r_square: 0.8451 - val_rmse: 0.2427    \n",
      "\n",
      "Epoch 78/150                                                                                                           \n",
      " - 0s - loss: 0.2702 - r_square: 0.7934 - rmse: 0.2702 - val_loss: 0.2610 - val_r_square: 0.8335 - val_rmse: 0.2610    \n",
      "\n",
      "Epoch 79/150                                                                                                           \n",
      " - 0s - loss: 0.2686 - r_square: 0.7997 - rmse: 0.2686 - val_loss: 0.2507 - val_r_square: 0.8445 - val_rmse: 0.2507    \n",
      "\n",
      "Epoch 80/150                                                                                                           \n",
      " - 0s - loss: 0.2694 - r_square: 0.7910 - rmse: 0.2694 - val_loss: 0.2428 - val_r_square: 0.8494 - val_rmse: 0.2428    \n",
      "\n",
      "Epoch 81/150                                                                                                           \n",
      " - 0s - loss: 0.2692 - r_square: 0.7999 - rmse: 0.2692 - val_loss: 0.2373 - val_r_square: 0.8528 - val_rmse: 0.2373    \n",
      "\n",
      "Epoch 82/150                                                                                                           \n",
      " - 0s - loss: 0.2617 - r_square: 0.8083 - rmse: 0.2617 - val_loss: 0.2482 - val_r_square: 0.8444 - val_rmse: 0.2482    \n",
      "\n",
      "Epoch 83/150                                                                                                           \n",
      " - 0s - loss: 0.2685 - r_square: 0.7921 - rmse: 0.2685 - val_loss: 0.2657 - val_r_square: 0.8325 - val_rmse: 0.2657    \n",
      "\n",
      "Epoch 84/150                                                                                                           \n",
      " - 0s - loss: 0.2696 - r_square: 0.7897 - rmse: 0.2696 - val_loss: 0.2532 - val_r_square: 0.8415 - val_rmse: 0.2532    \n",
      "\n",
      "Epoch 85/150                                                                                                           \n",
      " - 0s - loss: 0.2703 - r_square: 0.7955 - rmse: 0.2703 - val_loss: 0.2560 - val_r_square: 0.8401 - val_rmse: 0.2560    \n",
      "\n",
      "Epoch 86/150                                                                                                           \n",
      " - 0s - loss: 0.2661 - r_square: 0.8003 - rmse: 0.2661 - val_loss: 0.2457 - val_r_square: 0.8446 - val_rmse: 0.2457    \n",
      "\n",
      "Epoch 87/150                                                                                                           \n",
      " - 0s - loss: 0.2634 - r_square: 0.8100 - rmse: 0.2634 - val_loss: 0.2546 - val_r_square: 0.8388 - val_rmse: 0.2546    \n",
      "\n",
      "Epoch 88/150                                                                                                           \n",
      " - 0s - loss: 0.2675 - r_square: 0.7991 - rmse: 0.2675 - val_loss: 0.2451 - val_r_square: 0.8451 - val_rmse: 0.2451    \n",
      "\n",
      "Epoch 89/150                                                                                                           \n",
      " - 0s - loss: 0.2689 - r_square: 0.8013 - rmse: 0.2689 - val_loss: 0.2407 - val_r_square: 0.8515 - val_rmse: 0.2407    \n",
      "\n",
      "Epoch 90/150                                                                                                           \n",
      " - 0s - loss: 0.2679 - r_square: 0.8010 - rmse: 0.2679 - val_loss: 0.2415 - val_r_square: 0.8481 - val_rmse: 0.2415    \n",
      "\n",
      "Epoch 91/150                                                                                                           \n",
      " - 0s - loss: 0.2655 - r_square: 0.8043 - rmse: 0.2655 - val_loss: 0.2524 - val_r_square: 0.8438 - val_rmse: 0.2524    \n",
      "\n",
      "Epoch 92/150                                                                                                           \n",
      " - 0s - loss: 0.2663 - r_square: 0.7987 - rmse: 0.2663 - val_loss: 0.2356 - val_r_square: 0.8543 - val_rmse: 0.2356    \n",
      "\n",
      "Epoch 93/150                                                                                                           \n",
      " - 0s - loss: 0.2699 - r_square: 0.7934 - rmse: 0.2699 - val_loss: 0.2606 - val_r_square: 0.8305 - val_rmse: 0.2606    \n",
      "\n",
      "Epoch 94/150                                                                                                           \n",
      " - 0s - loss: 0.2626 - r_square: 0.8072 - rmse: 0.2626 - val_loss: 0.2439 - val_r_square: 0.8467 - val_rmse: 0.2439    \n",
      "\n",
      "Epoch 95/150                                                                                                           \n",
      " - 0s - loss: 0.2678 - r_square: 0.8046 - rmse: 0.2678 - val_loss: 0.2403 - val_r_square: 0.8480 - val_rmse: 0.2403    \n",
      "\n",
      "Epoch 96/150                                                                                                           \n",
      " - 0s - loss: 0.2654 - r_square: 0.7936 - rmse: 0.2654 - val_loss: 0.2363 - val_r_square: 0.8531 - val_rmse: 0.2363    \n",
      "\n",
      "Epoch 97/150                                                                                                           \n",
      " - 0s - loss: 0.2666 - r_square: 0.8038 - rmse: 0.2666 - val_loss: 0.2483 - val_r_square: 0.8434 - val_rmse: 0.2483    \n",
      "\n",
      "Epoch 98/150                                                                                                           \n",
      " - 0s - loss: 0.2669 - r_square: 0.8002 - rmse: 0.2669 - val_loss: 0.2446 - val_r_square: 0.8511 - val_rmse: 0.2446    \n",
      "\n",
      "Epoch 99/150                                                                                                           \n",
      " - 0s - loss: 0.2687 - r_square: 0.8013 - rmse: 0.2687 - val_loss: 0.2436 - val_r_square: 0.8474 - val_rmse: 0.2436    \n",
      "\n",
      "Epoch 100/150                                                                                                          \n",
      " - 0s - loss: 0.2689 - r_square: 0.8020 - rmse: 0.2689 - val_loss: 0.2346 - val_r_square: 0.8557 - val_rmse: 0.2346    \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/150                                                                                                          \n",
      " - 0s - loss: 0.2642 - r_square: 0.8028 - rmse: 0.2642 - val_loss: 0.2441 - val_r_square: 0.8456 - val_rmse: 0.2441    \n",
      "\n",
      "Epoch 102/150                                                                                                          \n",
      " - 0s - loss: 0.2643 - r_square: 0.8087 - rmse: 0.2643 - val_loss: 0.2463 - val_r_square: 0.8470 - val_rmse: 0.2463    \n",
      "\n",
      "Epoch 103/150                                                                                                          \n",
      " - 0s - loss: 0.2672 - r_square: 0.7957 - rmse: 0.2672 - val_loss: 0.2514 - val_r_square: 0.8418 - val_rmse: 0.2514    \n",
      "\n",
      "Epoch 104/150                                                                                                          \n",
      " - 0s - loss: 0.2656 - r_square: 0.8010 - rmse: 0.2656 - val_loss: 0.2562 - val_r_square: 0.8352 - val_rmse: 0.2562    \n",
      "\n",
      "Epoch 105/150                                                                                                          \n",
      " - 0s - loss: 0.2647 - r_square: 0.7996 - rmse: 0.2647 - val_loss: 0.2538 - val_r_square: 0.8410 - val_rmse: 0.2538    \n",
      "\n",
      "Epoch 106/150                                                                                                          \n",
      " - 0s - loss: 0.2598 - r_square: 0.8047 - rmse: 0.2598 - val_loss: 0.2536 - val_r_square: 0.8416 - val_rmse: 0.2536    \n",
      "\n",
      "Epoch 107/150                                                                                                          \n",
      " - 0s - loss: 0.2617 - r_square: 0.8095 - rmse: 0.2617 - val_loss: 0.2413 - val_r_square: 0.8494 - val_rmse: 0.2413    \n",
      "\n",
      "Epoch 108/150                                                                                                          \n",
      " - 0s - loss: 0.2616 - r_square: 0.8019 - rmse: 0.2616 - val_loss: 0.2426 - val_r_square: 0.8491 - val_rmse: 0.2426    \n",
      "\n",
      "Epoch 109/150                                                                                                          \n",
      " - 0s - loss: 0.2596 - r_square: 0.8068 - rmse: 0.2596 - val_loss: 0.2431 - val_r_square: 0.8472 - val_rmse: 0.2431    \n",
      "\n",
      "Epoch 110/150                                                                                                          \n",
      " - 0s - loss: 0.2625 - r_square: 0.8038 - rmse: 0.2625 - val_loss: 0.2553 - val_r_square: 0.8416 - val_rmse: 0.2553    \n",
      "\n",
      "Epoch 111/150                                                                                                          \n",
      " - 0s - loss: 0.2579 - r_square: 0.8111 - rmse: 0.2579 - val_loss: 0.2501 - val_r_square: 0.8424 - val_rmse: 0.2501    \n",
      "\n",
      "Epoch 112/150                                                                                                          \n",
      " - 0s - loss: 0.2662 - r_square: 0.8004 - rmse: 0.2662 - val_loss: 0.2529 - val_r_square: 0.8404 - val_rmse: 0.2529    \n",
      "\n",
      "Epoch 113/150                                                                                                          \n",
      " - 0s - loss: 0.2637 - r_square: 0.7993 - rmse: 0.2637 - val_loss: 0.2495 - val_r_square: 0.8467 - val_rmse: 0.2495    \n",
      "\n",
      "Epoch 114/150                                                                                                          \n",
      " - 0s - loss: 0.2623 - r_square: 0.8015 - rmse: 0.2623 - val_loss: 0.2377 - val_r_square: 0.8510 - val_rmse: 0.2377    \n",
      "\n",
      "Epoch 115/150                                                                                                          \n",
      " - 0s - loss: 0.2642 - r_square: 0.7998 - rmse: 0.2642 - val_loss: 0.2491 - val_r_square: 0.8428 - val_rmse: 0.2491    \n",
      "\n",
      "Epoch 116/150                                                                                                          \n",
      " - 0s - loss: 0.2577 - r_square: 0.8163 - rmse: 0.2577 - val_loss: 0.2525 - val_r_square: 0.8446 - val_rmse: 0.2525    \n",
      "\n",
      "Epoch 117/150                                                                                                          \n",
      " - 0s - loss: 0.2602 - r_square: 0.8084 - rmse: 0.2602 - val_loss: 0.2459 - val_r_square: 0.8437 - val_rmse: 0.2459    \n",
      "\n",
      "Epoch 118/150                                                                                                          \n",
      " - 0s - loss: 0.2623 - r_square: 0.8022 - rmse: 0.2623 - val_loss: 0.2475 - val_r_square: 0.8452 - val_rmse: 0.2475    \n",
      "\n",
      "Epoch 119/150                                                                                                          \n",
      " - 0s - loss: 0.2569 - r_square: 0.8085 - rmse: 0.2569 - val_loss: 0.2481 - val_r_square: 0.8403 - val_rmse: 0.2481    \n",
      "\n",
      "Epoch 120/150                                                                                                          \n",
      " - 0s - loss: 0.2568 - r_square: 0.8127 - rmse: 0.2568 - val_loss: 0.2523 - val_r_square: 0.8414 - val_rmse: 0.2523    \n",
      "\n",
      "Epoch 121/150                                                                                                          \n",
      " - 0s - loss: 0.2682 - r_square: 0.7997 - rmse: 0.2682 - val_loss: 0.2448 - val_r_square: 0.8463 - val_rmse: 0.2448    \n",
      "\n",
      "Epoch 122/150                                                                                                          \n",
      " - 0s - loss: 0.2593 - r_square: 0.8106 - rmse: 0.2593 - val_loss: 0.2390 - val_r_square: 0.8486 - val_rmse: 0.2390    \n",
      "\n",
      "Epoch 123/150                                                                                                          \n",
      " - 0s - loss: 0.2650 - r_square: 0.8014 - rmse: 0.2650 - val_loss: 0.2499 - val_r_square: 0.8433 - val_rmse: 0.2499    \n",
      "\n",
      "Epoch 124/150                                                                                                          \n",
      " - 0s - loss: 0.2564 - r_square: 0.8055 - rmse: 0.2564 - val_loss: 0.2452 - val_r_square: 0.8443 - val_rmse: 0.2452    \n",
      "\n",
      "Epoch 125/150                                                                                                          \n",
      " - 0s - loss: 0.2595 - r_square: 0.8048 - rmse: 0.2595 - val_loss: 0.2413 - val_r_square: 0.8481 - val_rmse: 0.2413    \n",
      "\n",
      "Epoch 126/150                                                                                                          \n",
      " - 0s - loss: 0.2563 - r_square: 0.8100 - rmse: 0.2563 - val_loss: 0.2408 - val_r_square: 0.8504 - val_rmse: 0.2408    \n",
      "\n",
      "Epoch 127/150                                                                                                          \n",
      " - 0s - loss: 0.2533 - r_square: 0.8203 - rmse: 0.2533 - val_loss: 0.2443 - val_r_square: 0.8500 - val_rmse: 0.2443    \n",
      "\n",
      "Epoch 128/150                                                                                                          \n",
      " - 0s - loss: 0.2531 - r_square: 0.8199 - rmse: 0.2531 - val_loss: 0.2459 - val_r_square: 0.8412 - val_rmse: 0.2459    \n",
      "\n",
      "Epoch 129/150                                                                                                          \n",
      " - 0s - loss: 0.2600 - r_square: 0.8051 - rmse: 0.2600 - val_loss: 0.2437 - val_r_square: 0.8470 - val_rmse: 0.2437    \n",
      "\n",
      "Epoch 130/150                                                                                                          \n",
      " - 0s - loss: 0.2572 - r_square: 0.8105 - rmse: 0.2572 - val_loss: 0.2498 - val_r_square: 0.8463 - val_rmse: 0.2498    \n",
      "\n",
      "Epoch 131/150                                                                                                          \n",
      " - 0s - loss: 0.2587 - r_square: 0.8091 - rmse: 0.2587 - val_loss: 0.2396 - val_r_square: 0.8521 - val_rmse: 0.2396    \n",
      "\n",
      "Epoch 132/150                                                                                                          \n",
      " - 0s - loss: 0.2604 - r_square: 0.8090 - rmse: 0.2604 - val_loss: 0.2511 - val_r_square: 0.8425 - val_rmse: 0.2511    \n",
      "\n",
      "Epoch 133/150                                                                                                          \n",
      " - 0s - loss: 0.2587 - r_square: 0.8098 - rmse: 0.2587 - val_loss: 0.2514 - val_r_square: 0.8402 - val_rmse: 0.2514    \n",
      "\n",
      "Epoch 134/150                                                                                                          \n",
      " - 0s - loss: 0.2607 - r_square: 0.8160 - rmse: 0.2607 - val_loss: 0.2397 - val_r_square: 0.8496 - val_rmse: 0.2397    \n",
      "\n",
      "Epoch 135/150                                                                                                          \n",
      " - 0s - loss: 0.2576 - r_square: 0.8057 - rmse: 0.2576 - val_loss: 0.2470 - val_r_square: 0.8430 - val_rmse: 0.2470    \n",
      "\n",
      "Epoch 136/150                                                                                                          \n",
      " - 0s - loss: 0.2627 - r_square: 0.7969 - rmse: 0.2627 - val_loss: 0.2449 - val_r_square: 0.8458 - val_rmse: 0.2449    \n",
      "\n",
      "Epoch 137/150                                                                                                          \n",
      " - 0s - loss: 0.2604 - r_square: 0.8001 - rmse: 0.2604 - val_loss: 0.2485 - val_r_square: 0.8422 - val_rmse: 0.2485    \n",
      "\n",
      "Epoch 138/150                                                                                                          \n",
      " - 0s - loss: 0.2616 - r_square: 0.8049 - rmse: 0.2616 - val_loss: 0.2414 - val_r_square: 0.8516 - val_rmse: 0.2414    \n",
      "\n",
      "Epoch 139/150                                                                                                          \n",
      " - 0s - loss: 0.2538 - r_square: 0.8103 - rmse: 0.2538 - val_loss: 0.2301 - val_r_square: 0.8590 - val_rmse: 0.2301    \n",
      "\n",
      "Epoch 140/150                                                                                                          \n",
      " - 0s - loss: 0.2608 - r_square: 0.8032 - rmse: 0.2608 - val_loss: 0.2448 - val_r_square: 0.8454 - val_rmse: 0.2448    \n",
      "\n",
      "Epoch 141/150                                                                                                          \n",
      " - 0s - loss: 0.2551 - r_square: 0.8101 - rmse: 0.2551 - val_loss: 0.2302 - val_r_square: 0.8564 - val_rmse: 0.2302    \n",
      "\n",
      "Epoch 142/150                                                                                                          \n",
      " - 0s - loss: 0.2573 - r_square: 0.8030 - rmse: 0.2573 - val_loss: 0.2393 - val_r_square: 0.8519 - val_rmse: 0.2393    \n",
      "\n",
      "Epoch 143/150                                                                                                          \n",
      " - 0s - loss: 0.2570 - r_square: 0.8137 - rmse: 0.2570 - val_loss: 0.2330 - val_r_square: 0.8570 - val_rmse: 0.2330    \n",
      "\n",
      "Epoch 144/150                                                                                                          \n",
      " - 0s - loss: 0.2580 - r_square: 0.8058 - rmse: 0.2580 - val_loss: 0.2398 - val_r_square: 0.8488 - val_rmse: 0.2398    \n",
      "\n",
      "Epoch 145/150                                                                                                          \n",
      " - 0s - loss: 0.2582 - r_square: 0.8096 - rmse: 0.2582 - val_loss: 0.2282 - val_r_square: 0.8590 - val_rmse: 0.2282    \n",
      "\n",
      "Epoch 146/150                                                                                                          \n",
      " - 0s - loss: 0.2572 - r_square: 0.8004 - rmse: 0.2572 - val_loss: 0.2349 - val_r_square: 0.8515 - val_rmse: 0.2349    \n",
      "\n",
      "Epoch 147/150                                                                                                          \n",
      " - 0s - loss: 0.2542 - r_square: 0.8201 - rmse: 0.2542 - val_loss: 0.2459 - val_r_square: 0.8437 - val_rmse: 0.2459    \n",
      "\n",
      "Epoch 148/150                                                                                                          \n",
      " - 0s - loss: 0.2614 - r_square: 0.8014 - rmse: 0.2614 - val_loss: 0.2373 - val_r_square: 0.8526 - val_rmse: 0.2373    \n",
      "\n",
      "Epoch 149/150                                                                                                          \n",
      " - 0s - loss: 0.2586 - r_square: 0.8026 - rmse: 0.2586 - val_loss: 0.2466 - val_r_square: 0.8481 - val_rmse: 0.2466    \n",
      "\n",
      "Epoch 150/150                                                                                                          \n",
      " - 0s - loss: 0.2594 - r_square: 0.8067 - rmse: 0.2594 - val_loss: 0.2401 - val_r_square: 0.8515 - val_rmse: 0.2401    \n",
      "\n",
      "Lowest Validation Loss:                                                                                                \n",
      "0.2282174607385256                                                                                                     \n",
      "(4457, 6)                                                                                                              \n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                       | 1/5 [00:17<01:09, 17.38s/trial, best loss: -0.2282174607385256]WARNING:tensorflow:Large dropout rate: 0.836667 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.912829 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_2\"                                                                                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "dense_4 (Dense)              (None, 500)               3500                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_3 (Dropout)          (None, 500)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_5 (Dense)              (None, 500)               250500                                                          \n",
      "_________________________________________________________________                                                      \n",
      "dropout_4 (Dropout)          (None, 500)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_6 (Dense)              (None, 1)                 501                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 254,501                                                                                                  \n",
      "Trainable params: 254,501                                                                                              \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "Train on 3788 samples, validate on 669 samples                                                                         \n",
      "Epoch 1/50                                                                                                             \n",
      " - 0s - loss: 0.8078 - r_square: -1.6520e-01 - rmse: 0.8078 - val_loss: 0.5236 - val_r_square: 0.5277 - val_rmse: 0.5236\n",
      "\n",
      "Epoch 2/50                                                                                                             \n",
      " - 0s - loss: 0.6280 - r_square: 0.2939 - rmse: 0.6280 - val_loss: 0.4996 - val_r_square: 0.5757 - val_rmse: 0.4996    \n",
      "\n",
      "Epoch 3/50                                                                                                             \n",
      " - 0s - loss: 0.5488 - r_square: 0.4307 - rmse: 0.5488 - val_loss: 0.4929 - val_r_square: 0.5887 - val_rmse: 0.4929    \n",
      "\n",
      "Epoch 4/50                                                                                                             \n",
      " - 0s - loss: 0.5102 - r_square: 0.5060 - rmse: 0.5102 - val_loss: 0.4382 - val_r_square: 0.6603 - val_rmse: 0.4382    \n",
      "\n",
      "Epoch 5/50                                                                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.4885 - r_square: 0.5352 - rmse: 0.4885 - val_loss: 0.4351 - val_r_square: 0.6605 - val_rmse: 0.4351    \n",
      "\n",
      "Epoch 6/50                                                                                                             \n",
      " - 0s - loss: 0.4644 - r_square: 0.5665 - rmse: 0.4644 - val_loss: 0.4314 - val_r_square: 0.6680 - val_rmse: 0.4314    \n",
      "\n",
      "Epoch 7/50                                                                                                             \n",
      " - 0s - loss: 0.4485 - r_square: 0.5960 - rmse: 0.4485 - val_loss: 0.3954 - val_r_square: 0.6995 - val_rmse: 0.3954    \n",
      "\n",
      "Epoch 8/50                                                                                                             \n",
      " - 0s - loss: 0.4511 - r_square: 0.5829 - rmse: 0.4511 - val_loss: 0.3909 - val_r_square: 0.7135 - val_rmse: 0.3909    \n",
      "\n",
      "Epoch 9/50                                                                                                             \n",
      " - 0s - loss: 0.4417 - r_square: 0.5951 - rmse: 0.4417 - val_loss: 0.3781 - val_r_square: 0.7213 - val_rmse: 0.3781    \n",
      "\n",
      "Epoch 10/50                                                                                                            \n",
      " - 0s - loss: 0.4195 - r_square: 0.6292 - rmse: 0.4195 - val_loss: 0.3447 - val_r_square: 0.7504 - val_rmse: 0.3447    \n",
      "\n",
      "Epoch 11/50                                                                                                            \n",
      " - 0s - loss: 0.4167 - r_square: 0.6350 - rmse: 0.4167 - val_loss: 0.3558 - val_r_square: 0.7442 - val_rmse: 0.3558    \n",
      "\n",
      "Epoch 12/50                                                                                                            \n",
      " - 0s - loss: 0.4123 - r_square: 0.6224 - rmse: 0.4123 - val_loss: 0.3382 - val_r_square: 0.7595 - val_rmse: 0.3382    \n",
      "\n",
      "Epoch 13/50                                                                                                            \n",
      " - 0s - loss: 0.4144 - r_square: 0.6268 - rmse: 0.4144 - val_loss: 0.3569 - val_r_square: 0.7492 - val_rmse: 0.3569    \n",
      "\n",
      "Epoch 14/50                                                                                                            \n",
      " - 0s - loss: 0.3990 - r_square: 0.6441 - rmse: 0.3990 - val_loss: 0.3507 - val_r_square: 0.7512 - val_rmse: 0.3507    \n",
      "\n",
      "Epoch 15/50                                                                                                            \n",
      " - 0s - loss: 0.3958 - r_square: 0.6577 - rmse: 0.3958 - val_loss: 0.3168 - val_r_square: 0.7773 - val_rmse: 0.3168    \n",
      "\n",
      "Epoch 16/50                                                                                                            \n",
      " - 0s - loss: 0.3851 - r_square: 0.6692 - rmse: 0.3851 - val_loss: 0.3122 - val_r_square: 0.7844 - val_rmse: 0.3122    \n",
      "\n",
      "Epoch 17/50                                                                                                            \n",
      " - 0s - loss: 0.3785 - r_square: 0.6895 - rmse: 0.3785 - val_loss: 0.2977 - val_r_square: 0.7925 - val_rmse: 0.2977    \n",
      "\n",
      "Epoch 18/50                                                                                                            \n",
      " - 0s - loss: 0.3788 - r_square: 0.6736 - rmse: 0.3788 - val_loss: 0.3055 - val_r_square: 0.7869 - val_rmse: 0.3055    \n",
      "\n",
      "Epoch 19/50                                                                                                            \n",
      " - 0s - loss: 0.3771 - r_square: 0.6824 - rmse: 0.3771 - val_loss: 0.2900 - val_r_square: 0.8000 - val_rmse: 0.2900    \n",
      "\n",
      "Epoch 20/50                                                                                                            \n",
      " - 0s - loss: 0.3686 - r_square: 0.6924 - rmse: 0.3686 - val_loss: 0.2904 - val_r_square: 0.7969 - val_rmse: 0.2904    \n",
      "\n",
      "Epoch 21/50                                                                                                            \n",
      " - 0s - loss: 0.3678 - r_square: 0.6807 - rmse: 0.3678 - val_loss: 0.2780 - val_r_square: 0.8046 - val_rmse: 0.2780    \n",
      "\n",
      "Epoch 22/50                                                                                                            \n",
      " - 0s - loss: 0.3690 - r_square: 0.6940 - rmse: 0.3690 - val_loss: 0.2874 - val_r_square: 0.8016 - val_rmse: 0.2874    \n",
      "\n",
      "Epoch 23/50                                                                                                            \n",
      " - 0s - loss: 0.3544 - r_square: 0.7016 - rmse: 0.3544 - val_loss: 0.2777 - val_r_square: 0.8119 - val_rmse: 0.2777    \n",
      "\n",
      "Epoch 24/50                                                                                                            \n",
      " - 0s - loss: 0.3614 - r_square: 0.7020 - rmse: 0.3614 - val_loss: 0.2746 - val_r_square: 0.8107 - val_rmse: 0.2746    \n",
      "\n",
      "Epoch 25/50                                                                                                            \n",
      " - 0s - loss: 0.3578 - r_square: 0.7047 - rmse: 0.3578 - val_loss: 0.2644 - val_r_square: 0.8186 - val_rmse: 0.2644    \n",
      "\n",
      "Epoch 26/50                                                                                                            \n",
      " - 0s - loss: 0.3551 - r_square: 0.7052 - rmse: 0.3551 - val_loss: 0.2632 - val_r_square: 0.8174 - val_rmse: 0.2632    \n",
      "\n",
      "Epoch 27/50                                                                                                            \n",
      " - 0s - loss: 0.3514 - r_square: 0.7075 - rmse: 0.3514 - val_loss: 0.2654 - val_r_square: 0.8187 - val_rmse: 0.2654    \n",
      "\n",
      "Epoch 28/50                                                                                                            \n",
      " - 0s - loss: 0.3552 - r_square: 0.7045 - rmse: 0.3552 - val_loss: 0.2732 - val_r_square: 0.8168 - val_rmse: 0.2732    \n",
      "\n",
      "Epoch 29/50                                                                                                            \n",
      " - 0s - loss: 0.3438 - r_square: 0.7297 - rmse: 0.3438 - val_loss: 0.2667 - val_r_square: 0.8217 - val_rmse: 0.2667    \n",
      "\n",
      "Epoch 30/50                                                                                                            \n",
      " - 0s - loss: 0.3426 - r_square: 0.7172 - rmse: 0.3426 - val_loss: 0.2564 - val_r_square: 0.8276 - val_rmse: 0.2564    \n",
      "\n",
      "Epoch 31/50                                                                                                            \n",
      " - 0s - loss: 0.3423 - r_square: 0.7186 - rmse: 0.3423 - val_loss: 0.2638 - val_r_square: 0.8255 - val_rmse: 0.2638    \n",
      "\n",
      "Epoch 32/50                                                                                                            \n",
      " - 0s - loss: 0.3426 - r_square: 0.7245 - rmse: 0.3426 - val_loss: 0.2611 - val_r_square: 0.8244 - val_rmse: 0.2611    \n",
      "\n",
      "Epoch 33/50                                                                                                            \n",
      " - 0s - loss: 0.3451 - r_square: 0.7153 - rmse: 0.3451 - val_loss: 0.2602 - val_r_square: 0.8253 - val_rmse: 0.2602    \n",
      "\n",
      "Epoch 34/50                                                                                                            \n",
      " - 0s - loss: 0.3404 - r_square: 0.7278 - rmse: 0.3404 - val_loss: 0.2679 - val_r_square: 0.8208 - val_rmse: 0.2679    \n",
      "\n",
      "Epoch 35/50                                                                                                            \n",
      " - 0s - loss: 0.3451 - r_square: 0.7237 - rmse: 0.3451 - val_loss: 0.2522 - val_r_square: 0.8262 - val_rmse: 0.2522    \n",
      "\n",
      "Epoch 36/50                                                                                                            \n",
      " - 0s - loss: 0.3411 - r_square: 0.7359 - rmse: 0.3411 - val_loss: 0.2532 - val_r_square: 0.8303 - val_rmse: 0.2532    \n",
      "\n",
      "Epoch 37/50                                                                                                            \n",
      " - 0s - loss: 0.3406 - r_square: 0.7278 - rmse: 0.3406 - val_loss: 0.2577 - val_r_square: 0.8314 - val_rmse: 0.2577    \n",
      "\n",
      "Epoch 38/50                                                                                                            \n",
      " - 0s - loss: 0.3417 - r_square: 0.7200 - rmse: 0.3417 - val_loss: 0.2500 - val_r_square: 0.8344 - val_rmse: 0.2500    \n",
      "\n",
      "Epoch 39/50                                                                                                            \n",
      " - 0s - loss: 0.3367 - r_square: 0.7319 - rmse: 0.3367 - val_loss: 0.2752 - val_r_square: 0.8225 - val_rmse: 0.2752    \n",
      "\n",
      "Epoch 40/50                                                                                                            \n",
      " - 0s - loss: 0.3382 - r_square: 0.7251 - rmse: 0.3382 - val_loss: 0.2456 - val_r_square: 0.8344 - val_rmse: 0.2456    \n",
      "\n",
      "Epoch 41/50                                                                                                            \n",
      " - 0s - loss: 0.3368 - r_square: 0.7311 - rmse: 0.3368 - val_loss: 0.2476 - val_r_square: 0.8347 - val_rmse: 0.2476    \n",
      "\n",
      "Epoch 42/50                                                                                                            \n",
      " - 0s - loss: 0.3364 - r_square: 0.7260 - rmse: 0.3364 - val_loss: 0.2511 - val_r_square: 0.8336 - val_rmse: 0.2511    \n",
      "\n",
      "Epoch 43/50                                                                                                            \n",
      " - 0s - loss: 0.3314 - r_square: 0.7405 - rmse: 0.3314 - val_loss: 0.2471 - val_r_square: 0.8370 - val_rmse: 0.2471    \n",
      "\n",
      "Epoch 44/50                                                                                                            \n",
      " - 0s - loss: 0.3280 - r_square: 0.7372 - rmse: 0.3280 - val_loss: 0.2456 - val_r_square: 0.8390 - val_rmse: 0.2456    \n",
      "\n",
      "Epoch 45/50                                                                                                            \n",
      " - 0s - loss: 0.3324 - r_square: 0.7323 - rmse: 0.3324 - val_loss: 0.2614 - val_r_square: 0.8304 - val_rmse: 0.2614    \n",
      "\n",
      "Epoch 46/50                                                                                                            \n",
      " - 0s - loss: 0.3308 - r_square: 0.7371 - rmse: 0.3308 - val_loss: 0.2495 - val_r_square: 0.8348 - val_rmse: 0.2495    \n",
      "\n",
      "Epoch 47/50                                                                                                            \n",
      " - 0s - loss: 0.3283 - r_square: 0.7434 - rmse: 0.3283 - val_loss: 0.2390 - val_r_square: 0.8437 - val_rmse: 0.2390    \n",
      "\n",
      "Epoch 48/50                                                                                                            \n",
      " - 0s - loss: 0.3282 - r_square: 0.7521 - rmse: 0.3282 - val_loss: 0.2480 - val_r_square: 0.8405 - val_rmse: 0.2480    \n",
      "\n",
      "Epoch 49/50                                                                                                            \n",
      " - 0s - loss: 0.3255 - r_square: 0.7460 - rmse: 0.3255 - val_loss: 0.2400 - val_r_square: 0.8401 - val_rmse: 0.2400    \n",
      "\n",
      "Epoch 50/50                                                                                                            \n",
      " - 0s - loss: 0.3311 - r_square: 0.7388 - rmse: 0.3311 - val_loss: 0.2343 - val_r_square: 0.8444 - val_rmse: 0.2343    \n",
      "\n",
      "Lowest Validation Loss:                                                                                                \n",
      "0.2343165476671427                                                                                                     \n",
      "(4457, 6)                                                                                                              \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 2/5 [00:35<00:52, 17.61s/trial, best loss: -0.2343165476671427]WARNING:tensorflow:Large dropout rate: 0.975819 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_3\"                                                                                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "dense_7 (Dense)              (None, 50)                350                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dropout_5 (Dropout)          (None, 50)                0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_8 (Dense)              (None, 500)               25500                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dropout_6 (Dropout)          (None, 500)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_9 (Dense)              (None, 1)                 501                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 26,351                                                                                                   \n",
      "Trainable params: 26,351                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "Train on 3788 samples, validate on 669 samples                                                                         \n",
      "Epoch 1/100                                                                                                            \n",
      " - 0s - loss: 0.9175 - r_square: -5.5098e-01 - rmse: 0.9175 - val_loss: 0.7723 - val_r_square: 0.0545 - val_rmse: 0.7723\n",
      "\n",
      "Epoch 2/100                                                                                                            \n",
      " - 0s - loss: 0.8588 - r_square: -2.7667e-01 - rmse: 0.8588 - val_loss: 0.7412 - val_r_square: 0.1229 - val_rmse: 0.7412\n",
      "\n",
      "Epoch 3/100                                                                                                            \n",
      " - 0s - loss: 0.8319 - r_square: -1.8633e-01 - rmse: 0.8319 - val_loss: 0.7547 - val_r_square: 0.0765 - val_rmse: 0.7547\n",
      "\n",
      "Epoch 4/100                                                                                                            \n",
      " - 0s - loss: 0.7931 - r_square: -7.1171e-02 - rmse: 0.7931 - val_loss: 0.7485 - val_r_square: 0.0989 - val_rmse: 0.7485\n",
      "\n",
      "Epoch 5/100                                                                                                            \n",
      " - 0s - loss: 0.7731 - r_square: -2.7430e-03 - rmse: 0.7731 - val_loss: 0.7488 - val_r_square: 0.1039 - val_rmse: 0.7488\n",
      "\n",
      "Epoch 6/100                                                                                                            \n",
      " - 0s - loss: 0.7621 - r_square: -3.4386e-03 - rmse: 0.7621 - val_loss: 0.7491 - val_r_square: 0.1016 - val_rmse: 0.7491\n",
      "\n",
      "Epoch 7/100                                                                                                            \n",
      " - 0s - loss: 0.7470 - r_square: 0.0751 - rmse: 0.7470 - val_loss: 0.7440 - val_r_square: 0.1194 - val_rmse: 0.7440    \n",
      "\n",
      "Epoch 8/100                                                                                                            \n",
      " - 0s - loss: 0.7466 - r_square: 0.0535 - rmse: 0.7466 - val_loss: 0.7536 - val_r_square: 0.0886 - val_rmse: 0.7536    \n",
      "\n",
      "Epoch 9/100                                                                                                            \n",
      " - 0s - loss: 0.7373 - r_square: 0.0496 - rmse: 0.7373 - val_loss: 0.7413 - val_r_square: 0.1291 - val_rmse: 0.7413    \n",
      "\n",
      "Epoch 10/100                                                                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.7332 - r_square: 0.0795 - rmse: 0.7332 - val_loss: 0.7427 - val_r_square: 0.1216 - val_rmse: 0.7427    \n",
      "\n",
      "Epoch 11/100                                                                                                           \n",
      " - 0s - loss: 0.7218 - r_square: 0.1101 - rmse: 0.7218 - val_loss: 0.7411 - val_r_square: 0.1282 - val_rmse: 0.7411    \n",
      "\n",
      "Epoch 12/100                                                                                                           \n",
      " - 0s - loss: 0.7201 - r_square: 0.1054 - rmse: 0.7201 - val_loss: 0.7431 - val_r_square: 0.1219 - val_rmse: 0.7431    \n",
      "\n",
      "Epoch 13/100                                                                                                           \n",
      " - 0s - loss: 0.7174 - r_square: 0.1074 - rmse: 0.7174 - val_loss: 0.7506 - val_r_square: 0.1083 - val_rmse: 0.7506    \n",
      "\n",
      "Epoch 14/100                                                                                                           \n",
      " - 0s - loss: 0.7165 - r_square: 0.1070 - rmse: 0.7165 - val_loss: 0.7627 - val_r_square: 0.0711 - val_rmse: 0.7627    \n",
      "\n",
      "Epoch 15/100                                                                                                           \n",
      " - 0s - loss: 0.6961 - r_square: 0.1572 - rmse: 0.6961 - val_loss: 0.7503 - val_r_square: 0.1048 - val_rmse: 0.7503    \n",
      "\n",
      "Epoch 16/100                                                                                                           \n",
      " - 0s - loss: 0.7016 - r_square: 0.1371 - rmse: 0.7016 - val_loss: 0.7483 - val_r_square: 0.1093 - val_rmse: 0.7483    \n",
      "\n",
      "Epoch 17/100                                                                                                           \n",
      " - 0s - loss: 0.7086 - r_square: 0.1225 - rmse: 0.7086 - val_loss: 0.7498 - val_r_square: 0.1051 - val_rmse: 0.7498    \n",
      "\n",
      "Epoch 18/100                                                                                                           \n",
      " - 0s - loss: 0.7003 - r_square: 0.1475 - rmse: 0.7003 - val_loss: 0.7525 - val_r_square: 0.0964 - val_rmse: 0.7525    \n",
      "\n",
      "Epoch 19/100                                                                                                           \n",
      " - 0s - loss: 0.6955 - r_square: 0.1488 - rmse: 0.6955 - val_loss: 0.7547 - val_r_square: 0.0896 - val_rmse: 0.7547    \n",
      "\n",
      "Epoch 20/100                                                                                                           \n",
      " - 0s - loss: 0.6891 - r_square: 0.1622 - rmse: 0.6891 - val_loss: 0.7489 - val_r_square: 0.1079 - val_rmse: 0.7489    \n",
      "\n",
      "Epoch 21/100                                                                                                           \n",
      " - 0s - loss: 0.6971 - r_square: 0.1329 - rmse: 0.6971 - val_loss: 0.7602 - val_r_square: 0.0794 - val_rmse: 0.7602    \n",
      "\n",
      "Epoch 22/100                                                                                                           \n",
      " - 0s - loss: 0.6876 - r_square: 0.1609 - rmse: 0.6876 - val_loss: 0.7659 - val_r_square: 0.0643 - val_rmse: 0.7659    \n",
      "\n",
      "Epoch 23/100                                                                                                           \n",
      " - 0s - loss: 0.6855 - r_square: 0.1770 - rmse: 0.6855 - val_loss: 0.7620 - val_r_square: 0.0732 - val_rmse: 0.7620    \n",
      "\n",
      "Epoch 24/100                                                                                                           \n",
      " - 0s - loss: 0.6840 - r_square: 0.1743 - rmse: 0.6840 - val_loss: 0.7644 - val_r_square: 0.0658 - val_rmse: 0.7644    \n",
      "\n",
      "Epoch 25/100                                                                                                           \n",
      " - 0s - loss: 0.6823 - r_square: 0.1527 - rmse: 0.6823 - val_loss: 0.7653 - val_r_square: 0.0632 - val_rmse: 0.7653    \n",
      "\n",
      "Epoch 26/100                                                                                                           \n",
      " - 0s - loss: 0.6832 - r_square: 0.1668 - rmse: 0.6832 - val_loss: 0.7653 - val_r_square: 0.0621 - val_rmse: 0.7653    \n",
      "\n",
      "Epoch 27/100                                                                                                           \n",
      " - 0s - loss: 0.6871 - r_square: 0.1640 - rmse: 0.6871 - val_loss: 0.7610 - val_r_square: 0.0744 - val_rmse: 0.7610    \n",
      "\n",
      "Epoch 28/100                                                                                                           \n",
      " - 0s - loss: 0.6896 - r_square: 0.1602 - rmse: 0.6896 - val_loss: 0.7628 - val_r_square: 0.0703 - val_rmse: 0.7628    \n",
      "\n",
      "Epoch 29/100                                                                                                           \n",
      " - 0s - loss: 0.6891 - r_square: 0.1628 - rmse: 0.6891 - val_loss: 0.7642 - val_r_square: 0.0677 - val_rmse: 0.7642    \n",
      "\n",
      "Epoch 30/100                                                                                                           \n",
      " - 0s - loss: 0.6816 - r_square: 0.1473 - rmse: 0.6816 - val_loss: 0.7614 - val_r_square: 0.0726 - val_rmse: 0.7614    \n",
      "\n",
      "Epoch 31/100                                                                                                           \n",
      " - 0s - loss: 0.6643 - r_square: 0.2106 - rmse: 0.6643 - val_loss: 0.7648 - val_r_square: 0.0640 - val_rmse: 0.7648    \n",
      "\n",
      "Epoch 32/100                                                                                                           \n",
      " - 0s - loss: 0.6648 - r_square: 0.2070 - rmse: 0.6648 - val_loss: 0.7689 - val_r_square: 0.0546 - val_rmse: 0.7689    \n",
      "\n",
      "Epoch 33/100                                                                                                           \n",
      " - 0s - loss: 0.6698 - r_square: 0.1798 - rmse: 0.6698 - val_loss: 0.7682 - val_r_square: 0.0560 - val_rmse: 0.7682    \n",
      "\n",
      "Epoch 34/100                                                                                                           \n",
      " - 0s - loss: 0.6656 - r_square: 0.2010 - rmse: 0.6656 - val_loss: 0.7675 - val_r_square: 0.0569 - val_rmse: 0.7675    \n",
      "\n",
      "Epoch 35/100                                                                                                           \n",
      " - 0s - loss: 0.6678 - r_square: 0.1946 - rmse: 0.6678 - val_loss: 0.7748 - val_r_square: 0.0400 - val_rmse: 0.7748    \n",
      "\n",
      "Epoch 36/100                                                                                                           \n",
      " - 0s - loss: 0.6669 - r_square: 0.1860 - rmse: 0.6669 - val_loss: 0.7723 - val_r_square: 0.0452 - val_rmse: 0.7723    \n",
      "\n",
      "Epoch 37/100                                                                                                           \n",
      " - 0s - loss: 0.6712 - r_square: 0.1996 - rmse: 0.6712 - val_loss: 0.7767 - val_r_square: 0.0354 - val_rmse: 0.7767    \n",
      "\n",
      "Epoch 38/100                                                                                                           \n",
      " - 0s - loss: 0.6594 - r_square: 0.2061 - rmse: 0.6594 - val_loss: 0.7731 - val_r_square: 0.0424 - val_rmse: 0.7731    \n",
      "\n",
      "Epoch 39/100                                                                                                           \n",
      " - 0s - loss: 0.6657 - r_square: 0.1690 - rmse: 0.6657 - val_loss: 0.7707 - val_r_square: 0.0487 - val_rmse: 0.7707    \n",
      "\n",
      "Epoch 40/100                                                                                                           \n",
      " - 0s - loss: 0.6656 - r_square: 0.1830 - rmse: 0.6656 - val_loss: 0.7749 - val_r_square: 0.0377 - val_rmse: 0.7749    \n",
      "\n",
      "Epoch 41/100                                                                                                           \n",
      " - 0s - loss: 0.6552 - r_square: 0.2233 - rmse: 0.6552 - val_loss: 0.7726 - val_r_square: 0.0420 - val_rmse: 0.7726    \n",
      "\n",
      "Epoch 42/100                                                                                                           \n",
      " - 0s - loss: 0.6618 - r_square: 0.1977 - rmse: 0.6618 - val_loss: 0.7785 - val_r_square: 0.0275 - val_rmse: 0.7785    \n",
      "\n",
      "Epoch 43/100                                                                                                           \n",
      " - 0s - loss: 0.6531 - r_square: 0.2185 - rmse: 0.6531 - val_loss: 0.7788 - val_r_square: 0.0254 - val_rmse: 0.7788    \n",
      "\n",
      "Epoch 44/100                                                                                                           \n",
      " - 0s - loss: 0.6516 - r_square: 0.2214 - rmse: 0.6516 - val_loss: 0.7784 - val_r_square: 0.0290 - val_rmse: 0.7784    \n",
      "\n",
      "Epoch 45/100                                                                                                           \n",
      " - 0s - loss: 0.6576 - r_square: 0.1973 - rmse: 0.6576 - val_loss: 0.7773 - val_r_square: 0.0324 - val_rmse: 0.7773    \n",
      "\n",
      "Epoch 46/100                                                                                                           \n",
      " - 0s - loss: 0.6543 - r_square: 0.2199 - rmse: 0.6543 - val_loss: 0.7768 - val_r_square: 0.0339 - val_rmse: 0.7768    \n",
      "\n",
      "Epoch 47/100                                                                                                           \n",
      " - 0s - loss: 0.6643 - r_square: 0.1968 - rmse: 0.6643 - val_loss: 0.7780 - val_r_square: 0.0319 - val_rmse: 0.7780    \n",
      "\n",
      "Epoch 48/100                                                                                                           \n",
      " - 0s - loss: 0.6545 - r_square: 0.2110 - rmse: 0.6545 - val_loss: 0.7778 - val_r_square: 0.0305 - val_rmse: 0.7778    \n",
      "\n",
      "Epoch 49/100                                                                                                           \n",
      " - 0s - loss: 0.6600 - r_square: 0.2060 - rmse: 0.6600 - val_loss: 0.7785 - val_r_square: 0.0295 - val_rmse: 0.7785    \n",
      "\n",
      "Epoch 50/100                                                                                                           \n",
      " - 0s - loss: 0.6663 - r_square: 0.1875 - rmse: 0.6663 - val_loss: 0.7763 - val_r_square: 0.0352 - val_rmse: 0.7763    \n",
      "\n",
      "Epoch 51/100                                                                                                           \n",
      " - 0s - loss: 0.6593 - r_square: 0.1928 - rmse: 0.6593 - val_loss: 0.7784 - val_r_square: 0.0307 - val_rmse: 0.7784    \n",
      "\n",
      "Epoch 52/100                                                                                                           \n",
      " - 0s - loss: 0.6555 - r_square: 0.2152 - rmse: 0.6555 - val_loss: 0.7755 - val_r_square: 0.0370 - val_rmse: 0.7755    \n",
      "\n",
      "Epoch 53/100                                                                                                           \n",
      " - 0s - loss: 0.6580 - r_square: 0.2023 - rmse: 0.6580 - val_loss: 0.7789 - val_r_square: 0.0281 - val_rmse: 0.7789    \n",
      "\n",
      "Epoch 54/100                                                                                                           \n",
      " - 0s - loss: 0.6530 - r_square: 0.2197 - rmse: 0.6530 - val_loss: 0.7772 - val_r_square: 0.0341 - val_rmse: 0.7772    \n",
      "\n",
      "Epoch 55/100                                                                                                           \n",
      " - 0s - loss: 0.6536 - r_square: 0.2210 - rmse: 0.6536 - val_loss: 0.7762 - val_r_square: 0.0369 - val_rmse: 0.7762    \n",
      "\n",
      "Epoch 56/100                                                                                                           \n",
      " - 0s - loss: 0.6440 - r_square: 0.2233 - rmse: 0.6440 - val_loss: 0.7812 - val_r_square: 0.0234 - val_rmse: 0.7812    \n",
      "\n",
      "Epoch 57/100                                                                                                           \n",
      " - 0s - loss: 0.6534 - r_square: 0.2049 - rmse: 0.6534 - val_loss: 0.7805 - val_r_square: 0.0245 - val_rmse: 0.7805    \n",
      "\n",
      "Epoch 58/100                                                                                                           \n",
      " - 0s - loss: 0.6552 - r_square: 0.2136 - rmse: 0.6552 - val_loss: 0.7823 - val_r_square: 0.0198 - val_rmse: 0.7823    \n",
      "\n",
      "Epoch 59/100                                                                                                           \n",
      " - 0s - loss: 0.6505 - r_square: 0.2188 - rmse: 0.6505 - val_loss: 0.7824 - val_r_square: 0.0207 - val_rmse: 0.7824    \n",
      "\n",
      "Epoch 60/100                                                                                                           \n",
      " - 0s - loss: 0.6604 - r_square: 0.1953 - rmse: 0.6604 - val_loss: 0.7853 - val_r_square: 0.0121 - val_rmse: 0.7853    \n",
      "\n",
      "Epoch 61/100                                                                                                           \n",
      " - 0s - loss: 0.6577 - r_square: 0.2053 - rmse: 0.6577 - val_loss: 0.7841 - val_r_square: 0.0157 - val_rmse: 0.7841    \n",
      "\n",
      "Epoch 62/100                                                                                                           \n",
      " - 0s - loss: 0.6467 - r_square: 0.2326 - rmse: 0.6467 - val_loss: 0.7826 - val_r_square: 0.0209 - val_rmse: 0.7826    \n",
      "\n",
      "Epoch 63/100                                                                                                           \n",
      " - 0s - loss: 0.6515 - r_square: 0.2112 - rmse: 0.6515 - val_loss: 0.7834 - val_r_square: 0.0189 - val_rmse: 0.7834    \n",
      "\n",
      "Epoch 64/100                                                                                                           \n",
      " - 0s - loss: 0.6581 - r_square: 0.1928 - rmse: 0.6581 - val_loss: 0.7877 - val_r_square: 0.0096 - val_rmse: 0.7877    \n",
      "\n",
      "Epoch 65/100                                                                                                           \n",
      " - 0s - loss: 0.6508 - r_square: 0.2219 - rmse: 0.6508 - val_loss: 0.7866 - val_r_square: 0.0115 - val_rmse: 0.7866    \n",
      "\n",
      "Epoch 66/100                                                                                                           \n",
      " - 0s - loss: 0.6610 - r_square: 0.2060 - rmse: 0.6610 - val_loss: 0.7891 - val_r_square: 0.0051 - val_rmse: 0.7891    \n",
      "\n",
      "Epoch 67/100                                                                                                           \n",
      " - 0s - loss: 0.6528 - r_square: 0.2094 - rmse: 0.6528 - val_loss: 0.7900 - val_r_square: 0.0035 - val_rmse: 0.7900    \n",
      "\n",
      "Epoch 68/100                                                                                                           \n",
      " - 0s - loss: 0.6452 - r_square: 0.2167 - rmse: 0.6452 - val_loss: 0.7873 - val_r_square: 0.0110 - val_rmse: 0.7873    \n",
      "\n",
      "Epoch 69/100                                                                                                           \n",
      " - 0s - loss: 0.6532 - r_square: 0.1990 - rmse: 0.6532 - val_loss: 0.7903 - val_r_square: 0.0039 - val_rmse: 0.7903    \n",
      "\n",
      "Epoch 70/100                                                                                                           \n",
      " - 0s - loss: 0.6452 - r_square: 0.2208 - rmse: 0.6452 - val_loss: 0.7892 - val_r_square: 0.0055 - val_rmse: 0.7892    \n",
      "\n",
      "Epoch 71/100                                                                                                           \n",
      " - 0s - loss: 0.6525 - r_square: 0.2178 - rmse: 0.6525 - val_loss: 0.7886 - val_r_square: 0.0080 - val_rmse: 0.7886    \n",
      "\n",
      "Epoch 72/100                                                                                                           \n",
      " - 0s - loss: 0.6477 - r_square: 0.2315 - rmse: 0.6477 - val_loss: 0.7880 - val_r_square: 0.0105 - val_rmse: 0.7880    \n",
      "\n",
      "Epoch 73/100                                                                                                           \n",
      " - 0s - loss: 0.6478 - r_square: 0.2239 - rmse: 0.6478 - val_loss: 0.7896 - val_r_square: 0.0070 - val_rmse: 0.7896    \n",
      "\n",
      "Epoch 74/100                                                                                                           \n",
      " - 0s - loss: 0.6575 - r_square: 0.2001 - rmse: 0.6575 - val_loss: 0.7886 - val_r_square: 0.0100 - val_rmse: 0.7886    \n",
      "\n",
      "Epoch 75/100                                                                                                           \n",
      " - 0s - loss: 0.6620 - r_square: 0.1849 - rmse: 0.6620 - val_loss: 0.7913 - val_r_square: 0.0040 - val_rmse: 0.7913    \n",
      "\n",
      "Epoch 76/100                                                                                                           \n",
      " - 0s - loss: 0.6320 - r_square: 0.2482 - rmse: 0.6320 - val_loss: 0.7883 - val_r_square: 0.0106 - val_rmse: 0.7883    \n",
      "\n",
      "Epoch 77/100                                                                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.6524 - r_square: 0.1684 - rmse: 0.6524 - val_loss: 0.7906 - val_r_square: 0.0064 - val_rmse: 0.7906    \n",
      "\n",
      "Epoch 78/100                                                                                                           \n",
      " - 0s - loss: 0.6512 - r_square: 0.2147 - rmse: 0.6512 - val_loss: 0.7931 - val_r_square: 0.0014 - val_rmse: 0.7931    \n",
      "\n",
      "Epoch 79/100                                                                                                           \n",
      " - 0s - loss: 0.6506 - r_square: 0.2145 - rmse: 0.6506 - val_loss: 0.7925 - val_r_square: 0.0038 - val_rmse: 0.7925    \n",
      "\n",
      "Epoch 80/100                                                                                                           \n",
      " - 0s - loss: 0.6452 - r_square: 0.2275 - rmse: 0.6452 - val_loss: 0.7917 - val_r_square: 0.0059 - val_rmse: 0.7917    \n",
      "\n",
      "Epoch 81/100                                                                                                           \n",
      " - 0s - loss: 0.6470 - r_square: 0.2188 - rmse: 0.6470 - val_loss: 0.7955 - val_r_square: -2.5304e-03 - val_rmse: 0.7955\n",
      "\n",
      "Epoch 82/100                                                                                                           \n",
      " - 0s - loss: 0.6421 - r_square: 0.2251 - rmse: 0.6421 - val_loss: 0.7961 - val_r_square: -4.0313e-03 - val_rmse: 0.7961\n",
      "\n",
      "Epoch 83/100                                                                                                           \n",
      " - 0s - loss: 0.6481 - r_square: 0.2067 - rmse: 0.6481 - val_loss: 0.7934 - val_r_square: 0.0021 - val_rmse: 0.7934    \n",
      "\n",
      "Epoch 84/100                                                                                                           \n",
      " - 0s - loss: 0.6444 - r_square: 0.2309 - rmse: 0.6444 - val_loss: 0.7944 - val_r_square: 8.8215e-04 - val_rmse: 0.7944\n",
      "\n",
      "Epoch 85/100                                                                                                           \n",
      " - 0s - loss: 0.6504 - r_square: 0.2156 - rmse: 0.6504 - val_loss: 0.7939 - val_r_square: 5.5710e-04 - val_rmse: 0.7939\n",
      "\n",
      "Epoch 86/100                                                                                                           \n",
      " - 0s - loss: 0.6518 - r_square: 0.2157 - rmse: 0.6518 - val_loss: 0.7953 - val_r_square: -1.3762e-03 - val_rmse: 0.7953\n",
      "\n",
      "Epoch 87/100                                                                                                           \n",
      " - 0s - loss: 0.6499 - r_square: 0.2195 - rmse: 0.6499 - val_loss: 0.7944 - val_r_square: 0.0012 - val_rmse: 0.7944    \n",
      "\n",
      "Epoch 88/100                                                                                                           \n",
      " - 0s - loss: 0.6517 - r_square: 0.2097 - rmse: 0.6517 - val_loss: 0.7945 - val_r_square: 0.0014 - val_rmse: 0.7945    \n",
      "\n",
      "Epoch 89/100                                                                                                           \n",
      " - 0s - loss: 0.6506 - r_square: 0.2237 - rmse: 0.6506 - val_loss: 0.7954 - val_r_square: -6.8906e-04 - val_rmse: 0.7954\n",
      "\n",
      "Epoch 90/100                                                                                                           \n",
      " - 0s - loss: 0.6437 - r_square: 0.2174 - rmse: 0.6437 - val_loss: 0.7951 - val_r_square: 7.0596e-04 - val_rmse: 0.7951\n",
      "\n",
      "Epoch 91/100                                                                                                           \n",
      " - 0s - loss: 0.6492 - r_square: 0.2125 - rmse: 0.6492 - val_loss: 0.7957 - val_r_square: -3.9130e-04 - val_rmse: 0.7957\n",
      "\n",
      "Epoch 92/100                                                                                                           \n",
      " - 0s - loss: 0.6475 - r_square: 0.2118 - rmse: 0.6475 - val_loss: 0.7953 - val_r_square: 0.0012 - val_rmse: 0.7953    \n",
      "\n",
      "Epoch 93/100                                                                                                           \n",
      " - 0s - loss: 0.6410 - r_square: 0.2295 - rmse: 0.6410 - val_loss: 0.7970 - val_r_square: -3.2338e-03 - val_rmse: 0.7970\n",
      "\n",
      "Epoch 94/100                                                                                                           \n",
      " - 0s - loss: 0.6445 - r_square: 0.2226 - rmse: 0.6445 - val_loss: 0.7973 - val_r_square: -2.3800e-03 - val_rmse: 0.7973\n",
      "\n",
      "Epoch 95/100                                                                                                           \n",
      " - 0s - loss: 0.6545 - r_square: 0.2111 - rmse: 0.6545 - val_loss: 0.7978 - val_r_square: -4.2635e-03 - val_rmse: 0.7978\n",
      "\n",
      "Epoch 96/100                                                                                                           \n",
      " - 0s - loss: 0.6379 - r_square: 0.2539 - rmse: 0.6379 - val_loss: 0.7974 - val_r_square: -2.7081e-03 - val_rmse: 0.7974\n",
      "\n",
      "Epoch 97/100                                                                                                           \n",
      " - 0s - loss: 0.6365 - r_square: 0.2346 - rmse: 0.6365 - val_loss: 0.7991 - val_r_square: -4.8986e-03 - val_rmse: 0.7991\n",
      "\n",
      "Epoch 98/100                                                                                                           \n",
      " - 0s - loss: 0.6478 - r_square: 0.2242 - rmse: 0.6478 - val_loss: 0.7962 - val_r_square: 0.0016 - val_rmse: 0.7962    \n",
      "\n",
      "Epoch 99/100                                                                                                           \n",
      " - 0s - loss: 0.6532 - r_square: 0.2102 - rmse: 0.6532 - val_loss: 0.7953 - val_r_square: 0.0028 - val_rmse: 0.7953    \n",
      "\n",
      "Epoch 100/100                                                                                                          \n",
      " - 0s - loss: 0.6464 - r_square: 0.2209 - rmse: 0.6464 - val_loss: 0.7971 - val_r_square: -1.1327e-03 - val_rmse: 0.7971\n",
      "\n",
      "Lowest Validation Loss:                                                                                                \n",
      "0.741097746586764                                                                                                      \n",
      "(4457, 6)                                                                                                              \n",
      "Model: \"sequential_4\"                                                                                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "dense_10 (Dense)             (None, 500)               3500                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_7 (Dropout)          (None, 500)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_11 (Dense)             (None, 50)                25050                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dropout_8 (Dropout)          (None, 50)                0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_12 (Dense)             (None, 1)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 28,601                                                                                                   \n",
      "Trainable params: 28,601                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "Train on 3788 samples, validate on 669 samples                                                                         \n",
      "Epoch 1/100                                                                                                            \n",
      " - 0s - loss: 0.7261 - r_square: 0.0867 - rmse: 0.7261 - val_loss: 0.5137 - val_r_square: 0.5147 - val_rmse: 0.5137    \n",
      "\n",
      "Epoch 2/100                                                                                                            \n",
      " - 0s - loss: 0.5812 - r_square: 0.3721 - rmse: 0.5812 - val_loss: 0.4788 - val_r_square: 0.6016 - val_rmse: 0.4788    \n",
      "\n",
      "Epoch 3/100                                                                                                            \n",
      " - 0s - loss: 0.5278 - r_square: 0.4801 - rmse: 0.5278 - val_loss: 0.4439 - val_r_square: 0.6614 - val_rmse: 0.4439    \n",
      "\n",
      "Epoch 4/100                                                                                                            \n",
      " - 0s - loss: 0.4889 - r_square: 0.5264 - rmse: 0.4889 - val_loss: 0.4102 - val_r_square: 0.6911 - val_rmse: 0.4102    \n",
      "\n",
      "Epoch 5/100                                                                                                            \n",
      " - 0s - loss: 0.4694 - r_square: 0.5607 - rmse: 0.4694 - val_loss: 0.3897 - val_r_square: 0.7148 - val_rmse: 0.3897    \n",
      "\n",
      "Epoch 6/100                                                                                                            \n",
      " - 0s - loss: 0.4556 - r_square: 0.5852 - rmse: 0.4556 - val_loss: 0.3856 - val_r_square: 0.7239 - val_rmse: 0.3856    \n",
      "\n",
      "Epoch 7/100                                                                                                            \n",
      " - 0s - loss: 0.4423 - r_square: 0.5998 - rmse: 0.4423 - val_loss: 0.3770 - val_r_square: 0.7304 - val_rmse: 0.3770    \n",
      "\n",
      "Epoch 8/100                                                                                                            \n",
      " - 0s - loss: 0.4329 - r_square: 0.6065 - rmse: 0.4329 - val_loss: 0.3638 - val_r_square: 0.7440 - val_rmse: 0.3638    \n",
      "\n",
      "Epoch 9/100                                                                                                            \n",
      " - 0s - loss: 0.4130 - r_square: 0.6458 - rmse: 0.4130 - val_loss: 0.3378 - val_r_square: 0.7692 - val_rmse: 0.3378    \n",
      "\n",
      "Epoch 10/100                                                                                                           \n",
      " - 0s - loss: 0.4092 - r_square: 0.6330 - rmse: 0.4092 - val_loss: 0.3242 - val_r_square: 0.7752 - val_rmse: 0.3242    \n",
      "\n",
      "Epoch 11/100                                                                                                           \n",
      " - 0s - loss: 0.4041 - r_square: 0.6390 - rmse: 0.4041 - val_loss: 0.3152 - val_r_square: 0.7747 - val_rmse: 0.3152    \n",
      "\n",
      "Epoch 12/100                                                                                                           \n",
      " - 0s - loss: 0.3942 - r_square: 0.6615 - rmse: 0.3942 - val_loss: 0.3202 - val_r_square: 0.7806 - val_rmse: 0.3202    \n",
      "\n",
      "Epoch 13/100                                                                                                           \n",
      " - 0s - loss: 0.3907 - r_square: 0.6626 - rmse: 0.3907 - val_loss: 0.3036 - val_r_square: 0.7912 - val_rmse: 0.3036    \n",
      "\n",
      "Epoch 14/100                                                                                                           \n",
      " - 0s - loss: 0.3866 - r_square: 0.6619 - rmse: 0.3866 - val_loss: 0.2983 - val_r_square: 0.7950 - val_rmse: 0.2983    \n",
      "\n",
      "Epoch 15/100                                                                                                           \n",
      " - 0s - loss: 0.3866 - r_square: 0.6686 - rmse: 0.3866 - val_loss: 0.2967 - val_r_square: 0.7960 - val_rmse: 0.2967    \n",
      "\n",
      "Epoch 16/100                                                                                                           \n",
      " - 0s - loss: 0.3728 - r_square: 0.6803 - rmse: 0.3728 - val_loss: 0.2909 - val_r_square: 0.8036 - val_rmse: 0.2909    \n",
      "\n",
      "Epoch 17/100                                                                                                           \n",
      " - 0s - loss: 0.3765 - r_square: 0.6760 - rmse: 0.3765 - val_loss: 0.2791 - val_r_square: 0.8094 - val_rmse: 0.2791    \n",
      "\n",
      "Epoch 18/100                                                                                                           \n",
      " - 0s - loss: 0.3720 - r_square: 0.6825 - rmse: 0.3720 - val_loss: 0.2654 - val_r_square: 0.8166 - val_rmse: 0.2654    \n",
      "\n",
      "Epoch 19/100                                                                                                           \n",
      " - 0s - loss: 0.3576 - r_square: 0.6993 - rmse: 0.3576 - val_loss: 0.2666 - val_r_square: 0.8193 - val_rmse: 0.2666    \n",
      "\n",
      "Epoch 20/100                                                                                                           \n",
      " - 0s - loss: 0.3585 - r_square: 0.6912 - rmse: 0.3585 - val_loss: 0.2803 - val_r_square: 0.8128 - val_rmse: 0.2803    \n",
      "\n",
      "Epoch 21/100                                                                                                           \n",
      " - 0s - loss: 0.3554 - r_square: 0.6966 - rmse: 0.3554 - val_loss: 0.2810 - val_r_square: 0.8155 - val_rmse: 0.2810    \n",
      "\n",
      "Epoch 22/100                                                                                                           \n",
      " - 0s - loss: 0.3594 - r_square: 0.6966 - rmse: 0.3594 - val_loss: 0.2637 - val_r_square: 0.8275 - val_rmse: 0.2637    \n",
      "\n",
      "Epoch 23/100                                                                                                           \n",
      " - 0s - loss: 0.3564 - r_square: 0.6985 - rmse: 0.3564 - val_loss: 0.2734 - val_r_square: 0.8217 - val_rmse: 0.2734    \n",
      "\n",
      "Epoch 24/100                                                                                                           \n",
      " - 0s - loss: 0.3549 - r_square: 0.7130 - rmse: 0.3549 - val_loss: 0.2746 - val_r_square: 0.8211 - val_rmse: 0.2746    \n",
      "\n",
      "Epoch 25/100                                                                                                           \n",
      " - 0s - loss: 0.3473 - r_square: 0.7104 - rmse: 0.3473 - val_loss: 0.2681 - val_r_square: 0.8225 - val_rmse: 0.2681    \n",
      "\n",
      "Epoch 26/100                                                                                                           \n",
      " - 0s - loss: 0.3396 - r_square: 0.7163 - rmse: 0.3396 - val_loss: 0.2494 - val_r_square: 0.8328 - val_rmse: 0.2494    \n",
      "\n",
      "Epoch 27/100                                                                                                           \n",
      " - 0s - loss: 0.3484 - r_square: 0.7105 - rmse: 0.3484 - val_loss: 0.2655 - val_r_square: 0.8272 - val_rmse: 0.2655    \n",
      "\n",
      "Epoch 28/100                                                                                                           \n",
      " - 0s - loss: 0.3450 - r_square: 0.7170 - rmse: 0.3450 - val_loss: 0.2498 - val_r_square: 0.8351 - val_rmse: 0.2498    \n",
      "\n",
      "Epoch 29/100                                                                                                           \n",
      " - 0s - loss: 0.3402 - r_square: 0.7289 - rmse: 0.3402 - val_loss: 0.2422 - val_r_square: 0.8395 - val_rmse: 0.2422    \n",
      "\n",
      "Epoch 30/100                                                                                                           \n",
      " - 0s - loss: 0.3317 - r_square: 0.7356 - rmse: 0.3317 - val_loss: 0.2343 - val_r_square: 0.8402 - val_rmse: 0.2343    \n",
      "\n",
      "Epoch 31/100                                                                                                           \n",
      " - 0s - loss: 0.3316 - r_square: 0.7393 - rmse: 0.3316 - val_loss: 0.2380 - val_r_square: 0.8433 - val_rmse: 0.2380    \n",
      "\n",
      "Epoch 32/100                                                                                                           \n",
      " - 0s - loss: 0.3384 - r_square: 0.7206 - rmse: 0.3384 - val_loss: 0.2408 - val_r_square: 0.8430 - val_rmse: 0.2408    \n",
      "\n",
      "Epoch 33/100                                                                                                           \n",
      " - 0s - loss: 0.3257 - r_square: 0.7433 - rmse: 0.3257 - val_loss: 0.2265 - val_r_square: 0.8480 - val_rmse: 0.2265    \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100                                                                                                           \n",
      " - 0s - loss: 0.3312 - r_square: 0.7374 - rmse: 0.3312 - val_loss: 0.2298 - val_r_square: 0.8461 - val_rmse: 0.2298    \n",
      "\n",
      "Epoch 35/100                                                                                                           \n",
      " - 0s - loss: 0.3308 - r_square: 0.7362 - rmse: 0.3308 - val_loss: 0.2237 - val_r_square: 0.8466 - val_rmse: 0.2237    \n",
      "\n",
      "Epoch 36/100                                                                                                           \n",
      " - 0s - loss: 0.3287 - r_square: 0.7368 - rmse: 0.3287 - val_loss: 0.2274 - val_r_square: 0.8452 - val_rmse: 0.2274    \n",
      "\n",
      "Epoch 37/100                                                                                                           \n",
      " - 0s - loss: 0.3255 - r_square: 0.7439 - rmse: 0.3255 - val_loss: 0.2302 - val_r_square: 0.8488 - val_rmse: 0.2302    \n",
      "\n",
      "Epoch 38/100                                                                                                           \n",
      " - 0s - loss: 0.3226 - r_square: 0.7399 - rmse: 0.3226 - val_loss: 0.2348 - val_r_square: 0.8463 - val_rmse: 0.2348    \n",
      "\n",
      "Epoch 39/100                                                                                                           \n",
      " - 0s - loss: 0.3297 - r_square: 0.7245 - rmse: 0.3297 - val_loss: 0.2313 - val_r_square: 0.8492 - val_rmse: 0.2313    \n",
      "\n",
      "Epoch 40/100                                                                                                           \n",
      " - 0s - loss: 0.3302 - r_square: 0.7308 - rmse: 0.3302 - val_loss: 0.2395 - val_r_square: 0.8460 - val_rmse: 0.2395    \n",
      "\n",
      "Epoch 41/100                                                                                                           \n",
      " - 0s - loss: 0.3249 - r_square: 0.7371 - rmse: 0.3249 - val_loss: 0.2336 - val_r_square: 0.8442 - val_rmse: 0.2336    \n",
      "\n",
      "Epoch 42/100                                                                                                           \n",
      " - 0s - loss: 0.3201 - r_square: 0.7459 - rmse: 0.3201 - val_loss: 0.2345 - val_r_square: 0.8447 - val_rmse: 0.2345    \n",
      "\n",
      "Epoch 43/100                                                                                                           \n",
      " - 0s - loss: 0.3242 - r_square: 0.7408 - rmse: 0.3242 - val_loss: 0.2355 - val_r_square: 0.8460 - val_rmse: 0.2355    \n",
      "\n",
      "Epoch 44/100                                                                                                           \n",
      " - 0s - loss: 0.3229 - r_square: 0.7454 - rmse: 0.3229 - val_loss: 0.2280 - val_r_square: 0.8494 - val_rmse: 0.2280    \n",
      "\n",
      "Epoch 45/100                                                                                                           \n",
      " - 0s - loss: 0.3133 - r_square: 0.7530 - rmse: 0.3133 - val_loss: 0.2165 - val_r_square: 0.8548 - val_rmse: 0.2165    \n",
      "\n",
      "Epoch 46/100                                                                                                           \n",
      " - 0s - loss: 0.3164 - r_square: 0.7512 - rmse: 0.3164 - val_loss: 0.2353 - val_r_square: 0.8494 - val_rmse: 0.2353    \n",
      "\n",
      "Epoch 47/100                                                                                                           \n",
      " - 0s - loss: 0.3171 - r_square: 0.7542 - rmse: 0.3171 - val_loss: 0.2359 - val_r_square: 0.8506 - val_rmse: 0.2359    \n",
      "\n",
      "Epoch 48/100                                                                                                           \n",
      " - 0s - loss: 0.3137 - r_square: 0.7563 - rmse: 0.3137 - val_loss: 0.2287 - val_r_square: 0.8532 - val_rmse: 0.2287    \n",
      "\n",
      "Epoch 49/100                                                                                                           \n",
      " - 0s - loss: 0.3166 - r_square: 0.7513 - rmse: 0.3166 - val_loss: 0.2351 - val_r_square: 0.8517 - val_rmse: 0.2351    \n",
      "\n",
      "Epoch 50/100                                                                                                           \n",
      " - 0s - loss: 0.3139 - r_square: 0.7482 - rmse: 0.3139 - val_loss: 0.2148 - val_r_square: 0.8569 - val_rmse: 0.2148    \n",
      "\n",
      "Epoch 51/100                                                                                                           \n",
      " - 0s - loss: 0.3151 - r_square: 0.7440 - rmse: 0.3151 - val_loss: 0.2300 - val_r_square: 0.8534 - val_rmse: 0.2300    \n",
      "\n",
      "Epoch 52/100                                                                                                           \n",
      " - 0s - loss: 0.3090 - r_square: 0.7612 - rmse: 0.3090 - val_loss: 0.2312 - val_r_square: 0.8501 - val_rmse: 0.2312    \n",
      "\n",
      "Epoch 53/100                                                                                                           \n",
      " - 0s - loss: 0.3128 - r_square: 0.7527 - rmse: 0.3128 - val_loss: 0.2203 - val_r_square: 0.8584 - val_rmse: 0.2203    \n",
      "\n",
      "Epoch 54/100                                                                                                           \n",
      " - 0s - loss: 0.3121 - r_square: 0.7563 - rmse: 0.3121 - val_loss: 0.2221 - val_r_square: 0.8554 - val_rmse: 0.2221    \n",
      "\n",
      "Epoch 55/100                                                                                                           \n",
      " - 0s - loss: 0.3115 - r_square: 0.7704 - rmse: 0.3115 - val_loss: 0.2281 - val_r_square: 0.8550 - val_rmse: 0.2281    \n",
      "\n",
      "Epoch 56/100                                                                                                           \n",
      " - 0s - loss: 0.3108 - r_square: 0.7490 - rmse: 0.3108 - val_loss: 0.2186 - val_r_square: 0.8573 - val_rmse: 0.2186    \n",
      "\n",
      "Epoch 57/100                                                                                                           \n",
      " - 0s - loss: 0.3151 - r_square: 0.7546 - rmse: 0.3151 - val_loss: 0.2265 - val_r_square: 0.8541 - val_rmse: 0.2265    \n",
      "\n",
      "Epoch 58/100                                                                                                           \n",
      " - 0s - loss: 0.3167 - r_square: 0.7499 - rmse: 0.3167 - val_loss: 0.2182 - val_r_square: 0.8565 - val_rmse: 0.2182    \n",
      "\n",
      "Epoch 59/100                                                                                                           \n",
      " - 0s - loss: 0.3118 - r_square: 0.7550 - rmse: 0.3118 - val_loss: 0.2326 - val_r_square: 0.8553 - val_rmse: 0.2326    \n",
      "\n",
      "Epoch 60/100                                                                                                           \n",
      " - 0s - loss: 0.3112 - r_square: 0.7559 - rmse: 0.3112 - val_loss: 0.2284 - val_r_square: 0.8581 - val_rmse: 0.2284    \n",
      "\n",
      "Epoch 61/100                                                                                                           \n",
      " - 0s - loss: 0.3133 - r_square: 0.7497 - rmse: 0.3133 - val_loss: 0.2240 - val_r_square: 0.8571 - val_rmse: 0.2240    \n",
      "\n",
      "Epoch 62/100                                                                                                           \n",
      " - 0s - loss: 0.3061 - r_square: 0.7594 - rmse: 0.3061 - val_loss: 0.2102 - val_r_square: 0.8592 - val_rmse: 0.2102    \n",
      "\n",
      "Epoch 63/100                                                                                                           \n",
      " - 0s - loss: 0.3130 - r_square: 0.7543 - rmse: 0.3130 - val_loss: 0.2326 - val_r_square: 0.8534 - val_rmse: 0.2326    \n",
      "\n",
      "Epoch 64/100                                                                                                           \n",
      " - 0s - loss: 0.3072 - r_square: 0.7665 - rmse: 0.3072 - val_loss: 0.2203 - val_r_square: 0.8584 - val_rmse: 0.2203    \n",
      "\n",
      "Epoch 65/100                                                                                                           \n",
      " - 0s - loss: 0.3049 - r_square: 0.7597 - rmse: 0.3049 - val_loss: 0.2247 - val_r_square: 0.8547 - val_rmse: 0.2247    \n",
      "\n",
      "Epoch 66/100                                                                                                           \n",
      " - 0s - loss: 0.3107 - r_square: 0.7498 - rmse: 0.3107 - val_loss: 0.2250 - val_r_square: 0.8556 - val_rmse: 0.2250    \n",
      "\n",
      "Epoch 67/100                                                                                                           \n",
      " - 0s - loss: 0.3071 - r_square: 0.7616 - rmse: 0.3071 - val_loss: 0.2239 - val_r_square: 0.8566 - val_rmse: 0.2239    \n",
      "\n",
      "Epoch 68/100                                                                                                           \n",
      " - 0s - loss: 0.3127 - r_square: 0.7494 - rmse: 0.3127 - val_loss: 0.2350 - val_r_square: 0.8528 - val_rmse: 0.2350    \n",
      "\n",
      "Epoch 69/100                                                                                                           \n",
      " - 0s - loss: 0.3063 - r_square: 0.7510 - rmse: 0.3063 - val_loss: 0.2177 - val_r_square: 0.8604 - val_rmse: 0.2177    \n",
      "\n",
      "Epoch 70/100                                                                                                           \n",
      " - 0s - loss: 0.3064 - r_square: 0.7546 - rmse: 0.3064 - val_loss: 0.2187 - val_r_square: 0.8609 - val_rmse: 0.2187    \n",
      "\n",
      "Epoch 71/100                                                                                                           \n",
      " - 0s - loss: 0.3079 - r_square: 0.7596 - rmse: 0.3079 - val_loss: 0.2110 - val_r_square: 0.8629 - val_rmse: 0.2110    \n",
      "\n",
      "Epoch 72/100                                                                                                           \n",
      " - 0s - loss: 0.3049 - r_square: 0.7621 - rmse: 0.3049 - val_loss: 0.2250 - val_r_square: 0.8575 - val_rmse: 0.2250    \n",
      "\n",
      "Epoch 73/100                                                                                                           \n",
      " - 0s - loss: 0.3070 - r_square: 0.7581 - rmse: 0.3070 - val_loss: 0.2193 - val_r_square: 0.8627 - val_rmse: 0.2193    \n",
      "\n",
      "Epoch 74/100                                                                                                           \n",
      " - 0s - loss: 0.3058 - r_square: 0.7632 - rmse: 0.3058 - val_loss: 0.2237 - val_r_square: 0.8586 - val_rmse: 0.2237    \n",
      "\n",
      "Epoch 75/100                                                                                                           \n",
      " - 0s - loss: 0.3094 - r_square: 0.7576 - rmse: 0.3094 - val_loss: 0.2149 - val_r_square: 0.8632 - val_rmse: 0.2149    \n",
      "\n",
      "Epoch 76/100                                                                                                           \n",
      " - 0s - loss: 0.3060 - r_square: 0.7600 - rmse: 0.3060 - val_loss: 0.2092 - val_r_square: 0.8656 - val_rmse: 0.2092    \n",
      "\n",
      "Epoch 77/100                                                                                                           \n",
      " - 0s - loss: 0.3074 - r_square: 0.7640 - rmse: 0.3074 - val_loss: 0.2203 - val_r_square: 0.8593 - val_rmse: 0.2203    \n",
      "\n",
      "Epoch 78/100                                                                                                           \n",
      " - 0s - loss: 0.3073 - r_square: 0.7638 - rmse: 0.3073 - val_loss: 0.2320 - val_r_square: 0.8557 - val_rmse: 0.2320    \n",
      "\n",
      "Epoch 79/100                                                                                                           \n",
      " - 0s - loss: 0.3033 - r_square: 0.7667 - rmse: 0.3033 - val_loss: 0.2262 - val_r_square: 0.8599 - val_rmse: 0.2262    \n",
      "\n",
      "Epoch 80/100                                                                                                           \n",
      " - 0s - loss: 0.3043 - r_square: 0.7671 - rmse: 0.3043 - val_loss: 0.2169 - val_r_square: 0.8609 - val_rmse: 0.2169    \n",
      "\n",
      "Epoch 81/100                                                                                                           \n",
      " - 0s - loss: 0.3046 - r_square: 0.7530 - rmse: 0.3046 - val_loss: 0.2263 - val_r_square: 0.8586 - val_rmse: 0.2263    \n",
      "\n",
      "Epoch 82/100                                                                                                           \n",
      " - 0s - loss: 0.2990 - r_square: 0.7707 - rmse: 0.2990 - val_loss: 0.2147 - val_r_square: 0.8636 - val_rmse: 0.2147    \n",
      "\n",
      "Epoch 83/100                                                                                                           \n",
      " - 0s - loss: 0.3048 - r_square: 0.7557 - rmse: 0.3048 - val_loss: 0.2052 - val_r_square: 0.8693 - val_rmse: 0.2052    \n",
      "\n",
      "Epoch 84/100                                                                                                           \n",
      " - 0s - loss: 0.3060 - r_square: 0.7689 - rmse: 0.3060 - val_loss: 0.2210 - val_r_square: 0.8606 - val_rmse: 0.2210    \n",
      "\n",
      "Epoch 85/100                                                                                                           \n",
      " - 0s - loss: 0.3057 - r_square: 0.7583 - rmse: 0.3057 - val_loss: 0.2221 - val_r_square: 0.8637 - val_rmse: 0.2221    \n",
      "\n",
      "Epoch 86/100                                                                                                           \n",
      " - 0s - loss: 0.2954 - r_square: 0.7704 - rmse: 0.2954 - val_loss: 0.2121 - val_r_square: 0.8664 - val_rmse: 0.2121    \n",
      "\n",
      "Epoch 87/100                                                                                                           \n",
      " - 0s - loss: 0.3027 - r_square: 0.7589 - rmse: 0.3027 - val_loss: 0.2039 - val_r_square: 0.8657 - val_rmse: 0.2039    \n",
      "\n",
      "Epoch 88/100                                                                                                           \n",
      " - 0s - loss: 0.3020 - r_square: 0.7702 - rmse: 0.3020 - val_loss: 0.2028 - val_r_square: 0.8685 - val_rmse: 0.2028    \n",
      "\n",
      "Epoch 89/100                                                                                                           \n",
      " - 0s - loss: 0.3055 - r_square: 0.7694 - rmse: 0.3055 - val_loss: 0.2239 - val_r_square: 0.8609 - val_rmse: 0.2239    \n",
      "\n",
      "Epoch 90/100                                                                                                           \n",
      " - 0s - loss: 0.3002 - r_square: 0.7654 - rmse: 0.3002 - val_loss: 0.2211 - val_r_square: 0.8640 - val_rmse: 0.2211    \n",
      "\n",
      "Epoch 91/100                                                                                                           \n",
      " - 0s - loss: 0.3047 - r_square: 0.7642 - rmse: 0.3047 - val_loss: 0.2102 - val_r_square: 0.8669 - val_rmse: 0.2102    \n",
      "\n",
      "Epoch 92/100                                                                                                           \n",
      " - 0s - loss: 0.3039 - r_square: 0.7645 - rmse: 0.3039 - val_loss: 0.2160 - val_r_square: 0.8654 - val_rmse: 0.2160    \n",
      "\n",
      "Epoch 93/100                                                                                                           \n",
      " - 0s - loss: 0.2970 - r_square: 0.7735 - rmse: 0.2970 - val_loss: 0.2029 - val_r_square: 0.8705 - val_rmse: 0.2029    \n",
      "\n",
      "Epoch 94/100                                                                                                           \n",
      " - 0s - loss: 0.3018 - r_square: 0.7689 - rmse: 0.3018 - val_loss: 0.2259 - val_r_square: 0.8631 - val_rmse: 0.2259    \n",
      "\n",
      "Epoch 95/100                                                                                                           \n",
      " - 0s - loss: 0.3043 - r_square: 0.7579 - rmse: 0.3043 - val_loss: 0.2167 - val_r_square: 0.8663 - val_rmse: 0.2167    \n",
      "\n",
      "Epoch 96/100                                                                                                           \n",
      " - 0s - loss: 0.3070 - r_square: 0.7593 - rmse: 0.3070 - val_loss: 0.2159 - val_r_square: 0.8666 - val_rmse: 0.2159    \n",
      "\n",
      "Epoch 97/100                                                                                                           \n",
      " - 0s - loss: 0.2971 - r_square: 0.7682 - rmse: 0.2971 - val_loss: 0.2154 - val_r_square: 0.8659 - val_rmse: 0.2154    \n",
      "\n",
      "Epoch 98/100                                                                                                           \n",
      " - 0s - loss: 0.2942 - r_square: 0.7786 - rmse: 0.2942 - val_loss: 0.2066 - val_r_square: 0.8687 - val_rmse: 0.2066    \n",
      "\n",
      "Epoch 99/100                                                                                                           \n",
      " - 0s - loss: 0.2956 - r_square: 0.7699 - rmse: 0.2956 - val_loss: 0.2141 - val_r_square: 0.8663 - val_rmse: 0.2141    \n",
      "\n",
      "Epoch 100/100                                                                                                          \n",
      " - 0s - loss: 0.2959 - r_square: 0.7713 - rmse: 0.2959 - val_loss: 0.2100 - val_r_square: 0.8680 - val_rmse: 0.2100    \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest Validation Loss:                                                                                                \n",
      "0.20280466334523997                                                                                                    \n",
      "(4457, 6)                                                                                                              \n",
      "Model: \"sequential_5\"                                                                                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "dense_13 (Dense)             (None, 500)               3500                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_9 (Dropout)          (None, 500)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_14 (Dense)             (None, 200)               100200                                                          \n",
      "_________________________________________________________________                                                      \n",
      "dropout_10 (Dropout)         (None, 200)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_15 (Dense)             (None, 1)                 201                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 103,901                                                                                                  \n",
      "Trainable params: 103,901                                                                                              \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "Train on 3788 samples, validate on 669 samples                                                                         \n",
      "Epoch 1/50                                                                                                             \n",
      " - 0s - loss: 0.4397 - r_square: 0.5838 - rmse: 0.4397 - val_loss: 0.2899 - val_r_square: 0.7952 - val_rmse: 0.2899    \n",
      "\n",
      "Epoch 2/50                                                                                                             \n",
      " - 0s - loss: 0.2862 - r_square: 0.7656 - rmse: 0.2862 - val_loss: 0.2334 - val_r_square: 0.8270 - val_rmse: 0.2334    \n",
      "\n",
      "Epoch 3/50                                                                                                             \n",
      " - 0s - loss: 0.2461 - r_square: 0.8069 - rmse: 0.2461 - val_loss: 0.2050 - val_r_square: 0.8437 - val_rmse: 0.2050    \n",
      "\n",
      "Epoch 4/50                                                                                                             \n",
      " - 0s - loss: 0.2277 - r_square: 0.8165 - rmse: 0.2277 - val_loss: 0.2198 - val_r_square: 0.8259 - val_rmse: 0.2198    \n",
      "\n",
      "Epoch 5/50                                                                                                             \n",
      " - 0s - loss: 0.2219 - r_square: 0.8197 - rmse: 0.2219 - val_loss: 0.1845 - val_r_square: 0.8609 - val_rmse: 0.1845    \n",
      "\n",
      "Epoch 6/50                                                                                                             \n",
      " - 0s - loss: 0.2092 - r_square: 0.8305 - rmse: 0.2092 - val_loss: 0.1779 - val_r_square: 0.8584 - val_rmse: 0.1779    \n",
      "\n",
      "Epoch 7/50                                                                                                             \n",
      " - 0s - loss: 0.2040 - r_square: 0.8281 - rmse: 0.2040 - val_loss: 0.1965 - val_r_square: 0.8540 - val_rmse: 0.1965    \n",
      "\n",
      "Epoch 8/50                                                                                                             \n",
      " - 0s - loss: 0.2027 - r_square: 0.8316 - rmse: 0.2027 - val_loss: 0.1804 - val_r_square: 0.8590 - val_rmse: 0.1804    \n",
      "\n",
      "Epoch 9/50                                                                                                             \n",
      " - 0s - loss: 0.1915 - r_square: 0.8404 - rmse: 0.1915 - val_loss: 0.1758 - val_r_square: 0.8809 - val_rmse: 0.1758    \n",
      "\n",
      "Epoch 10/50                                                                                                            \n",
      " - 0s - loss: 0.1878 - r_square: 0.8427 - rmse: 0.1878 - val_loss: 0.1613 - val_r_square: 0.8848 - val_rmse: 0.1613    \n",
      "\n",
      "Epoch 11/50                                                                                                            \n",
      " - 0s - loss: 0.1825 - r_square: 0.8349 - rmse: 0.1825 - val_loss: 0.1525 - val_r_square: 0.8821 - val_rmse: 0.1525    \n",
      "\n",
      "Epoch 12/50                                                                                                            \n",
      " - 0s - loss: 0.1859 - r_square: 0.8466 - rmse: 0.1859 - val_loss: 0.1834 - val_r_square: 0.8770 - val_rmse: 0.1834    \n",
      "\n",
      "Epoch 13/50                                                                                                            \n",
      " - 0s - loss: 0.1867 - r_square: 0.8473 - rmse: 0.1867 - val_loss: 0.1625 - val_r_square: 0.8883 - val_rmse: 0.1625    \n",
      "\n",
      "Epoch 14/50                                                                                                            \n",
      " - 0s - loss: 0.1822 - r_square: 0.8394 - rmse: 0.1822 - val_loss: 0.1537 - val_r_square: 0.8926 - val_rmse: 0.1537    \n",
      "\n",
      "Epoch 15/50                                                                                                            \n",
      " - 0s - loss: 0.1812 - r_square: 0.8526 - rmse: 0.1812 - val_loss: 0.1637 - val_r_square: 0.8853 - val_rmse: 0.1637    \n",
      "\n",
      "Epoch 16/50                                                                                                            \n",
      " - 0s - loss: 0.1738 - r_square: 0.8494 - rmse: 0.1738 - val_loss: 0.1471 - val_r_square: 0.8983 - val_rmse: 0.1471    \n",
      "\n",
      "Epoch 17/50                                                                                                            \n",
      " - 0s - loss: 0.1734 - r_square: 0.8534 - rmse: 0.1734 - val_loss: 0.1575 - val_r_square: 0.8922 - val_rmse: 0.1575    \n",
      "\n",
      "Epoch 18/50                                                                                                            \n",
      " - 0s - loss: 0.1774 - r_square: 0.8523 - rmse: 0.1774 - val_loss: 0.1552 - val_r_square: 0.8869 - val_rmse: 0.1552    \n",
      "\n",
      "Epoch 19/50                                                                                                            \n",
      " - 0s - loss: 0.1733 - r_square: 0.8641 - rmse: 0.1733 - val_loss: 0.1453 - val_r_square: 0.8946 - val_rmse: 0.1453    \n",
      "\n",
      "Epoch 20/50                                                                                                            \n",
      " - 0s - loss: 0.1707 - r_square: 0.8532 - rmse: 0.1707 - val_loss: 0.1426 - val_r_square: 0.8990 - val_rmse: 0.1426    \n",
      "\n",
      "Epoch 21/50                                                                                                            \n",
      " - 0s - loss: 0.1705 - r_square: 0.8510 - rmse: 0.1705 - val_loss: 0.1390 - val_r_square: 0.8996 - val_rmse: 0.1390    \n",
      "\n",
      "Epoch 22/50                                                                                                            \n",
      " - 0s - loss: 0.1669 - r_square: 0.8563 - rmse: 0.1669 - val_loss: 0.1598 - val_r_square: 0.8903 - val_rmse: 0.1598    \n",
      "\n",
      "Epoch 23/50                                                                                                            \n",
      " - 0s - loss: 0.1720 - r_square: 0.8596 - rmse: 0.1720 - val_loss: 0.1449 - val_r_square: 0.9032 - val_rmse: 0.1449    \n",
      "\n",
      "Epoch 24/50                                                                                                            \n",
      " - 0s - loss: 0.1662 - r_square: 0.8577 - rmse: 0.1662 - val_loss: 0.1406 - val_r_square: 0.8928 - val_rmse: 0.1406    \n",
      "\n",
      "Epoch 25/50                                                                                                            \n",
      " - 0s - loss: 0.1623 - r_square: 0.8624 - rmse: 0.1623 - val_loss: 0.1491 - val_r_square: 0.8974 - val_rmse: 0.1491    \n",
      "\n",
      "Epoch 26/50                                                                                                            \n",
      " - 0s - loss: 0.1668 - r_square: 0.8641 - rmse: 0.1668 - val_loss: 0.1602 - val_r_square: 0.8911 - val_rmse: 0.1602    \n",
      "\n",
      "Epoch 27/50                                                                                                            \n",
      " - 0s - loss: 0.1662 - r_square: 0.8606 - rmse: 0.1662 - val_loss: 0.1415 - val_r_square: 0.9022 - val_rmse: 0.1415    \n",
      "\n",
      "Epoch 28/50                                                                                                            \n",
      " - 0s - loss: 0.1628 - r_square: 0.8574 - rmse: 0.1628 - val_loss: 0.1439 - val_r_square: 0.8982 - val_rmse: 0.1439    \n",
      "\n",
      "Epoch 29/50                                                                                                            \n",
      " - 0s - loss: 0.1595 - r_square: 0.8647 - rmse: 0.1595 - val_loss: 0.1382 - val_r_square: 0.8992 - val_rmse: 0.1382    \n",
      "\n",
      "Epoch 30/50                                                                                                            \n",
      " - 0s - loss: 0.1585 - r_square: 0.8650 - rmse: 0.1585 - val_loss: 0.1394 - val_r_square: 0.9051 - val_rmse: 0.1394    \n",
      "\n",
      "Epoch 31/50                                                                                                            \n",
      " - 0s - loss: 0.1601 - r_square: 0.8655 - rmse: 0.1601 - val_loss: 0.1369 - val_r_square: 0.8981 - val_rmse: 0.1369    \n",
      "\n",
      "Epoch 32/50                                                                                                            \n",
      " - 0s - loss: 0.1618 - r_square: 0.8610 - rmse: 0.1618 - val_loss: 0.1284 - val_r_square: 0.9061 - val_rmse: 0.1284    \n",
      "\n",
      "Epoch 33/50                                                                                                            \n",
      " - 0s - loss: 0.1530 - r_square: 0.8686 - rmse: 0.1530 - val_loss: 0.1329 - val_r_square: 0.9063 - val_rmse: 0.1329    \n",
      "\n",
      "Epoch 34/50                                                                                                            \n",
      " - 0s - loss: 0.1612 - r_square: 0.8644 - rmse: 0.1612 - val_loss: 0.1357 - val_r_square: 0.9035 - val_rmse: 0.1357    \n",
      "\n",
      "Epoch 35/50                                                                                                            \n",
      " - 0s - loss: 0.1566 - r_square: 0.8695 - rmse: 0.1566 - val_loss: 0.1322 - val_r_square: 0.9039 - val_rmse: 0.1322    \n",
      "\n",
      "Epoch 36/50                                                                                                            \n",
      " - 0s - loss: 0.1564 - r_square: 0.8716 - rmse: 0.1564 - val_loss: 0.1261 - val_r_square: 0.9095 - val_rmse: 0.1261    \n",
      "\n",
      "Epoch 37/50                                                                                                            \n",
      " - 0s - loss: 0.1537 - r_square: 0.8705 - rmse: 0.1537 - val_loss: 0.1306 - val_r_square: 0.9111 - val_rmse: 0.1306    \n",
      "\n",
      "Epoch 38/50                                                                                                            \n",
      " - 0s - loss: 0.1559 - r_square: 0.8725 - rmse: 0.1559 - val_loss: 0.1346 - val_r_square: 0.9094 - val_rmse: 0.1346    \n",
      "\n",
      "Epoch 39/50                                                                                                            \n",
      " - 0s - loss: 0.1565 - r_square: 0.8662 - rmse: 0.1565 - val_loss: 0.1293 - val_r_square: 0.9047 - val_rmse: 0.1293    \n",
      "\n",
      "Epoch 40/50                                                                                                            \n",
      " - 0s - loss: 0.1559 - r_square: 0.8640 - rmse: 0.1559 - val_loss: 0.1337 - val_r_square: 0.9060 - val_rmse: 0.1337    \n",
      "\n",
      "Epoch 41/50                                                                                                            \n",
      " - 0s - loss: 0.1538 - r_square: 0.8683 - rmse: 0.1538 - val_loss: 0.1273 - val_r_square: 0.9100 - val_rmse: 0.1273    \n",
      "\n",
      "Epoch 42/50                                                                                                            \n",
      " - 0s - loss: 0.1551 - r_square: 0.8756 - rmse: 0.1551 - val_loss: 0.1415 - val_r_square: 0.9069 - val_rmse: 0.1415    \n",
      "\n",
      "Epoch 43/50                                                                                                            \n",
      " - 0s - loss: 0.1588 - r_square: 0.8660 - rmse: 0.1588 - val_loss: 0.1291 - val_r_square: 0.9092 - val_rmse: 0.1291    \n",
      "\n",
      "Epoch 44/50                                                                                                            \n",
      " - 0s - loss: 0.1537 - r_square: 0.8741 - rmse: 0.1537 - val_loss: 0.1361 - val_r_square: 0.9043 - val_rmse: 0.1361    \n",
      "\n",
      "Epoch 45/50                                                                                                            \n",
      " - 0s - loss: 0.1549 - r_square: 0.8666 - rmse: 0.1549 - val_loss: 0.1392 - val_r_square: 0.9064 - val_rmse: 0.1392    \n",
      "\n",
      "Epoch 46/50                                                                                                            \n",
      " - 0s - loss: 0.1531 - r_square: 0.8604 - rmse: 0.1531 - val_loss: 0.1382 - val_r_square: 0.9067 - val_rmse: 0.1382    \n",
      "\n",
      "Epoch 47/50                                                                                                            \n",
      " - 0s - loss: 0.1510 - r_square: 0.8752 - rmse: 0.1510 - val_loss: 0.1243 - val_r_square: 0.9117 - val_rmse: 0.1243    \n",
      "\n",
      "Epoch 48/50                                                                                                            \n",
      " - 0s - loss: 0.1503 - r_square: 0.8756 - rmse: 0.1503 - val_loss: 0.1247 - val_r_square: 0.9110 - val_rmse: 0.1247    \n",
      "\n",
      "Epoch 49/50                                                                                                            \n",
      " - 0s - loss: 0.1498 - r_square: 0.8723 - rmse: 0.1498 - val_loss: 0.1354 - val_r_square: 0.9038 - val_rmse: 0.1354    \n",
      "\n",
      "Epoch 50/50                                                                                                            \n",
      " - 0s - loss: 0.1497 - r_square: 0.8787 - rmse: 0.1497 - val_loss: 0.1293 - val_r_square: 0.9083 - val_rmse: 0.1293    \n",
      "\n",
      "Lowest Validation Loss:                                                                                                \n",
      "0.12427925488873982                                                                                                    \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:14<00:00, 14.84s/trial, best loss: -0.741097746586764]\n",
      "1486/1486 [==============================] - 0s 15us/step\n",
      "Evaluate: 0.8095009679107614\n",
      "Best Performing Model: {'Dense': 50, 'Dense_1': 500, 'Dropout': 0.9758185183456943, 'Dropout_1': 0.288662535902546, 'batch_size': 64, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "#defining the create model function\n",
    "exec('from __future__ import absolute_import, division, print_function')\n",
    "from hyperas.distributions import uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from keras import backend as K\n",
    "\n",
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    print(x_train.shape)\n",
    "    model= Sequential() \n",
    "    model.add(Dense({{choice([50,200,500])}}, input_dim=x_train.shape[1], activation= 'relu'))\n",
    "    model.add(Dropout({{uniform(0,1)}}))\n",
    "    model.add(Dense({{choice([50,200,500])}},activation= 'relu'))\n",
    "    #model.add(Activation('relu'))\n",
    "    model.add(Dropout({{uniform(0,1)}}))\n",
    "    model.add(Dense(1, activation= 'linear'))\n",
    "\n",
    "    \n",
    "################################################\n",
    "# CREDIT: https://github.com/keras-team/keras/issues/7947\n",
    "    def rmse(y_true, y_pred):\n",
    "        from keras import backend\n",
    "        return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "# mean squared error (mse) for regression  (only for Keras tensors)\n",
    "    def mse(y_true, y_pred):\n",
    "        from keras import backend\n",
    "        return backend.mean(backend.square(y_pred - y_true), axis=-1)\n",
    "    def r_square(y_true, y_pred):\n",
    "        SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "        SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "        return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "#############################################\n",
    "\n",
    "    model.compile(loss='mean_absolute_error', optimizer= 'adam', metrics=[r_square, rmse])\n",
    "    from keras.utils import print_summary\n",
    "    print_summary(model, line_length=None, positions=None, print_fn=None)\n",
    "    result= model.fit(x_train, y_train,\n",
    "                      batch_size={{choice([64,128])}},\n",
    "                      epochs={{choice([50,100,150])}},\n",
    "                      verbose=2,\n",
    "                      validation_split =0.15)\n",
    "    validation_acc= np.min(result.history['val_loss'])\n",
    "    print('Lowest Validation Loss:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}   \n",
    "\n",
    "#finding the best model\n",
    "best_run, best_model= optim.minimize(model=create_model,\n",
    "                                     data=data,\n",
    "                                     algo=tpe.suggest,\n",
    "                                     max_evals=5,\n",
    "                                     trials=Trials(),\n",
    "                                     eval_space=True,\n",
    "                                     notebook_name='NeuralAnalysis')\n",
    "score= best_model.evaluate(X_test_scaled,y_test_scaled, batch_size= 64)\n",
    "\n",
    "#predictions = best_model.predict(X_test_scaled)\n",
    "\n",
    "#print best model results\n",
    "print('Evaluate:', score[0])\n",
    "#print('Predictions:', predictions[:6])\n",
    "print('Best Performing Model:', best_run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11042935],\n",
       "       [-0.02757291],\n",
       "       [ 0.08504621],\n",
       "       ...,\n",
       "       [-0.08176664],\n",
       "       [-0.07878926],\n",
       "       [-0.07380855]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using model to predict y values\n",
    "predictions = best_model.predict(X_test_scaled)\n",
    "predictions1 = best_model.predict(X_train_scaled)\n",
    "predictions1\n",
    "#predictions= test\n",
    "#predictions1= train \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Residuals\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Neural Network Residual Plot')\n",
    "plt.scatter(predictions1, predictions1 - y_train_scaled, c= \"orange\",label=\"Training Data\", s=4)\n",
    "plt.scatter(predictions, predictions - y_test_scaled, c= \"blue\",label=\"Testing Data\",s=4)\n",
    "plt.ylim(-2,2)\n",
    "plt.hlines(y=0, xmin=predictions.min(), xmax=predictions.max())\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('neuralnetworkresidual.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Volve",
   "language": "python",
   "name": "volve"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
